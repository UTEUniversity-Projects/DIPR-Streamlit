import streamlit as st
import cv2
import numpy as np
import time
import tempfile
import os
from typing import List, Dict, Tuple, Optional

@st.cache_resource
def load_face_detector():
    from utils.face_utils import FaceDetector
    return FaceDetector("models/face_detection_yunet_2023mar.onnx")

@st.cache_resource
def load_face_recognizer():
    from utils.face_utils import FaceRecognizer
    return FaceRecognizer("models/face_recognition_sface_2021dec.onnx", "data/db_embeddings.pkl")

@st.cache_resource
def load_face_tracker():
    from utils.face_utils import StableFaceTracker
    return StableFaceTracker(max_distance=40.0, min_frames=2, max_missing_frames=8)

def show():
    # Th√™m ph·∫ßn gi·ªõi thi·ªáu v√† h∆∞·ªõng d·∫´n
    with st.expander("üîç Gi·ªõi thi·ªáu v·ªÅ nh·∫≠n d·∫°ng khu√¥n m·∫∑t", expanded=False):
        st.markdown("""
        ### Gi·ªõi thi·ªáu v·ªÅ nh·∫≠n d·∫°ng khu√¥n m·∫∑t
        
        T√≠nh nƒÉng nh·∫≠n d·∫°ng khu√¥n m·∫∑t trong ·ª©ng d·ª•ng n√†y s·ª≠ d·ª•ng ki·∫øn tr√∫c hai giai ƒëo·∫°n hi·ªán ƒë·∫°i:
        
        1. **YuNet Face Detector**: M√¥ h√¨nh ph√°t hi·ªán khu√¥n m·∫∑t d·ª±a tr√™n CNN (Convolutional Neural Network) ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a
           - Ki·∫øn tr√∫c: Lightweight network v·ªõi c√°c kh·ªëi ResNet c·∫£i ti·∫øn
           - Hi·ªáu su·∫•t: ƒê·ªô ch√≠nh x√°c cao v√† x·ª≠ l√Ω th·ªùi gian th·ª±c tr√™n CPU
           - ƒê·ªãnh d·∫°ng: ONNX v·ªõi k√≠ch th∆∞·ªõc nh·ªè (< 1MB)
        
        2. **SFace Recognition**: M√¥ h√¨nh nh·∫≠n d·∫°ng khu√¥n m·∫∑t v·ªõi ƒë·ªô ch√≠nh x√°c cao
           - Ki·∫øn tr√∫c: D·ª±a tr√™n m·∫°ng ResNet-50 c·∫£i ti·∫øn cho c√°c embedding vector 128 chi·ªÅu
           - Training: ƒê∆∞·ª£c hu·∫•n luy·ªán tr√™n dataset l·ªõn v·ªõi loss function ƒë·∫∑c bi·ªát gi√∫p ph√¢n bi·ªát t·ªët c√°c khu√¥n m·∫∑t
           - Ph∆∞∆°ng ph√°p: So s√°nh vector embedding v·ªõi c√°c vector trong c∆° s·ªü d·ªØ li·ªáu
        
        ·ª®ng d·ª•ng c≈©ng t√≠ch h·ª£p thu·∫≠t to√°n theo d√µi khu√¥n m·∫∑t (face tracking) ƒë·ªÉ ƒë·∫£m b·∫£o ·ªïn ƒë·ªãnh khi nh·∫≠n d·∫°ng video th·ªùi gian th·ª±c, gi·∫£m hi·ªán t∆∞·ª£ng "ch·ªõp nh√°y" khi x√°c ƒë·ªãnh danh t√≠nh.
        
        **·ª®ng d·ª•ng th·ª±c t·∫ø:**
        - H·ªá th·ªëng b·∫£o m·∫≠t v√† ki·ªÉm so√°t truy c·∫≠p
        - H·ªá th·ªëng ƒëi·ªÉm danh t·ª± ƒë·ªông
        - Tr·∫£i nghi·ªám c√° nh√¢n h√≥a trong c√°c h·ªá th·ªëng th√¥ng minh
        - X√°c th·ª±c danh t√≠nh kh√¥ng ti·∫øp x√∫c
        """)
            
    with st.expander("üìã H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng", expanded=False):
        st.markdown("""
        ### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng
        
        #### 1. Ch·∫ø ƒë·ªô ·∫£nh tƒ©nh
        - **Upload ·∫£nh**: T·∫£i l√™n ·∫£nh ch·ª©a khu√¥n m·∫∑t c·∫ßn nh·∫≠n d·∫°ng
        - **Ch·ª•p t·ª´ webcam**: Ch·ª•p ·∫£nh tr·ª±c ti·∫øp t·ª´ webcam ƒë·ªÉ nh·∫≠n d·∫°ng
        
        #### 2. Ch·∫ø ƒë·ªô video
        - **Video tr·ª±c ti·∫øp**: S·ª≠ d·ª•ng webcam ƒë·ªÉ nh·∫≠n d·∫°ng khu√¥n m·∫∑t theo th·ªùi gian th·ª±c
        - **Upload video**: T·∫£i l√™n v√† ph√¢n t√≠ch video c√≥ s·∫µn ƒë·ªÉ nh·∫≠n d·∫°ng khu√¥n m·∫∑t
        
        #### 3. T√πy ch·ªânh x·ª≠ l√Ω video
        - **T·ªëc ƒë·ªô x·ª≠ l√Ω**: ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô x·ª≠ l√Ω khung h√¨nh ƒë·ªÉ c√¢n b·∫±ng gi·ªØa hi·ªáu su·∫•t v√† ƒë·ªô ch√≠nh x√°c
        - **ƒê·ªô ph√¢n gi·∫£i**: Ch·ªçn ƒë·ªô ph√¢n gi·∫£i ph√π h·ª£p v·ªõi nhu c·∫ßu
        - **Ph√¢n t√≠ch video**: T√πy ch·ªânh m·ª©c ƒë·ªô chi ti·∫øt trong ph√¢n t√≠ch video
        
        #### M·∫πo s·ª≠ d·ª•ng:
        - **√Ånh s√°ng**: ƒê·∫£m b·∫£o khu√¥n m·∫∑t ƒë∆∞·ª£c chi·∫øu s√°ng t·ªët
        - **G√≥c nh√¨n**: H∆∞·ªõng m·∫∑t th·∫≥ng v√†o camera ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët nh·∫•t
        - **Kho·∫£ng c√°ch**: ƒê·ª©ng c√°ch camera kho·∫£ng 0.5-1m
        - **ƒêƒÉng k√Ω m·∫∑t m·ªõi**: N·∫øu khu√¥n m·∫∑t ch∆∞a ƒë∆∞·ª£c nh·∫≠n d·∫°ng, s·ª≠ d·ª•ng ch·ª©c nƒÉng "ƒêƒÉng k√Ω khu√¥n m·∫∑t m·ªõi"
        
        #### X·ª≠ l√Ω l·ªói:
        - **Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c**: ƒê·∫£m b·∫£o khu√¥n m·∫∑t ƒë√£ ƒë∆∞·ª£c ƒëƒÉng k√Ω trong h·ªá th·ªëng
        - **Nh·∫≠n d·∫°ng sai**: C·∫≠p nh·∫≠t database v·ªõi nhi·ªÅu m·∫´u khu√¥n m·∫∑t h∆°n
        - **Kh√¥ng hi·ªÉn th·ªã camera**: Ki·ªÉm tra quy·ªÅn truy c·∫≠p camera trong tr√¨nh duy·ªát
        """)
    
    # Choose input mode
    mode = st.radio("Ch·ªçn ch·∫ø ƒë·ªô nh·∫≠n d·∫°ng:", ["üì∏ ·∫¢nh tƒ©nh", "üé• Video tr·ª±c ti·∫øp", "üìπ Video upload"])
    
    # Load models
    face_detector = load_face_detector()
    face_recognizer = load_face_recognizer()
    
    if mode == "üì∏ ·∫¢nh tƒ©nh":
        # Image input methods
        input_method = st.radio("Ch·ªçn ngu·ªìn ·∫£nh:", ["Upload ·∫£nh", "Ch·ª•p t·ª´ webcam"])
        
        if input_method == "Ch·ª•p t·ª´ webcam":
            # Camera capture interface  
            picture = st.camera_input("Ch·ª•p ·∫£nh khu√¥n m·∫∑t")
            
            if picture:
                # Convert to OpenCV format
                bytes_data = picture.getvalue()
                img = cv2.imdecode(np.frombuffer(bytes_data, np.uint8), cv2.IMREAD_COLOR)
                
                # Create two columns for original and result images
                col1, col2 = st.columns(2)
                
                # Display original image
                with col1:
                    st.subheader("·∫¢nh g·ªëc")
                    st.image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), use_container_width=True)
                
                # Process image
                with st.spinner("ƒêang nh·∫≠n d·∫°ng khu√¥n m·∫∑t..."):
                    faces, aligned_faces = face_detector.detect(img)
                    
                    if len(faces) > 0:
                        names = []
                        scores = []
                        
                        # Recognize faces (limit to 5)
                        for face_img in aligned_faces[:5]:
                            name, score = face_recognizer.identify(face_img, 0.6)  # Set threshold to 0.6
                            names.append(name)
                            scores.append(score)
                        
                        # Draw results
                        from utils.face_utils import draw_results
                        result_img = draw_results(img, faces[:5], names, scores)
                    else:
                        result_img = img  # No faces detected
                        names = []
                        scores = []
                    
                    # Display result
                    with col2:
                        st.subheader("K·∫øt qu·∫£")
                        st.image(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB), use_container_width=True)
                    
                    # Display information below images - only show high confidence results
                    has_high_confidence = False
                    if names:
                        st.markdown("### Th√¥ng tin nh·∫≠n d·∫°ng:")
                        for i, (name, score) in enumerate(zip(names, scores)):
                            if score >= 0.6:  # Only show results with confidence >= 0.6
                                has_high_confidence = True
                                status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if name != "Unknown" else "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c"
                                st.write(f"**Ng∆∞·ªùi {i+1}:** {name} ({score:.2f}) - {status}")
                        
                        if not has_high_confidence:
                            st.info("Kh√¥ng c√≥ khu√¥n m·∫∑t n√†o ƒë∆∞·ª£c nh·∫≠n d·∫°ng v·ªõi ƒë·ªô tin c·∫≠y > 0.6")
                    else:
                        st.warning("Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t n√†o trong ·∫£nh!")
        
        else:  # Upload image mode
            uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh ch·ª©a khu√¥n m·∫∑t", type=["jpg", "jpeg", "png", "bmp"])
            
            if uploaded_file is not None:
                # Read image
                try:
                    file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
                    img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
                    
                    # Check if image was loaded successfully
                    if img is None:
                        st.error("Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh. Vui l√≤ng th·ª≠ file kh√°c.")
                    else:
                        # Create two columns for original and result images
                        col1, col2 = st.columns(2)
                        
                        # Display original image
                        with col1:
                            st.subheader("·∫¢nh g·ªëc")
                            st.image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), use_container_width=True)
                        
                        # Process image
                        with st.spinner("ƒêang nh·∫≠n d·∫°ng khu√¥n m·∫∑t..."):
                            faces, aligned_faces = face_detector.detect(img)
                            
                            if len(faces) > 0:
                                names = []
                                scores = []
                                
                                # Recognize faces (limit to 5)
                                for face_img in aligned_faces[:5]:
                                    name, score = face_recognizer.identify(face_img, 0.6)  # Threshold 0.6
                                    names.append(name)
                                    scores.append(score)
                                
                                # Draw results
                                from utils.face_utils import draw_results
                                result_img = draw_results(img, faces[:5], names, scores)
                            else:
                                result_img = img  # No faces detected
                                names = []
                                scores = []
                            
                            # Display result
                            with col2:
                                st.subheader("K·∫øt qu·∫£")
                                st.image(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB), use_container_width=True)
                            
                            # Display information below images - only show high confidence results
                            has_high_confidence = False
                            if names:
                                st.markdown("### Th√¥ng tin nh·∫≠n d·∫°ng:")
                                for i, (name, score) in enumerate(zip(names, scores)):
                                    if score >= 0.6:  # Only show results with confidence >= 0.6
                                        has_high_confidence = True
                                        status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if name != "Unknown" else "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c"
                                        st.write(f"**Ng∆∞·ªùi {i+1}:** {name} ({score:.2f}) - {status}")
                                
                                if not has_high_confidence:
                                    st.info("Kh√¥ng c√≥ khu√¥n m·∫∑t n√†o ƒë∆∞·ª£c nh·∫≠n d·∫°ng v·ªõi ƒë·ªô tin c·∫≠y > 0.6")
                            else:
                                st.warning("Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t n√†o trong ·∫£nh!")
                except Exception as e:
                    st.error(f"L·ªói khi x·ª≠ l√Ω ·∫£nh: {str(e)}")
    
    elif mode == "üé• Video tr·ª±c ti·∫øp":
        st.markdown("### üé• Video tr·ª±c ti·∫øp t·ª´ camera")
        
        # Performance settings
        col1, col2 = st.columns(2)
        with col1:
            resolution = st.selectbox("ƒê·ªô ph√¢n gi·∫£i:", 
                                    ["640x480", "800x600", "1280x720"],
                                    index=0)
        with col2:
            process_rate = st.slider("T·ªëc ƒë·ªô x·ª≠ l√Ω:", 1, 6, 2, 1,
                                   help="Gi√° tr·ªã c√†ng cao, t·ªëc ƒë·ªô x·ª≠ l√Ω c√†ng nhanh nh∆∞ng c√≥ th·ªÉ b·ªè l·ª° m·ªôt s·ªë khung h√¨nh")
        
        # Parse resolution
        w, h = map(int, resolution.split('x'))
        
        # Control buttons
        col1, col2 = st.columns([1, 4])
        with col1:
            start_button = st.button("B·∫Øt ƒë·∫ßu", type="primary", use_container_width=True)
        with col2:
            stop_button = st.button("D·ª´ng", use_container_width=True)
        
        # Initialize session state
        if 'video_running' not in st.session_state:
            st.session_state.video_running = False
        
        # Initialize face tracker (faster settings)
        face_tracker = load_face_tracker()
        
        # Video display area
        video_placeholder = st.empty()
        info_placeholder = st.empty()
        
        # Start video stream
        if start_button:
            st.session_state.video_running = True
        
        if stop_button:
            st.session_state.video_running = False
        
        # Video processing loop
        if st.session_state.video_running:
            # Hi·ªÉn th·ªã loading state
            video_placeholder.markdown("### ‚è≥ ƒêang m·ªü camera...")
            
            # Try different camera indices
            cap = None
            for idx in range(3):  # Try index 0, 1, 2
                cap = cv2.VideoCapture(idx)
                if cap.isOpened():
                    break
                cap.release()
            
            if cap is None or not cap.isOpened():
                st.error("Kh√¥ng th·ªÉ m·ªü camera. Vui l√≤ng ki·ªÉm tra l·∫°i k·∫øt n·ªëi camera.")
            else:
                # Set camera resolution
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, w)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, h)
                cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                
                # Hi·ªÉn th·ªã th√†nh c√¥ng
                video_placeholder.markdown("‚úÖ Camera ƒë√£ s·∫µn s√†ng")
                time.sleep(0.5)  # Hi·ªÉn th·ªã th√¥ng b√°o trong 0.5 gi√¢y
                
                # FPS counter and frame processing counter
                fps_start_time = time.time()
                fps_counter = 0
                current_fps = 0
                frame_num = 0
                
                # Initialize variable to track last processed frame
                result_frame = None
                names = []
                scores = []
                
                # Performance optimization - use thread settings
                import threading
                if "last_process_time" not in st.session_state:
                    st.session_state.last_process_time = 0
                    
                process_interval = 0.05  # 50ms minimum between processing
                last_info_update = 0  # Track when we last updated info display
                
                try:
                    while st.session_state.video_running:
                        ret, frame = cap.read()
                        
                        if not ret:
                            st.error("Kh√¥ng th·ªÉ ƒë·ªçc khung h√¨nh t·ª´ camera!")
                            break
                        
                        # Flip frame horizontally for mirror effect
                        frame = cv2.flip(frame, 1)
                        
                        # Calculate current FPS
                        fps_counter += 1
                        current_time = time.time()
                        if current_time - fps_start_time >= 1.0:
                            current_fps = fps_counter
                            fps_counter = 0
                            fps_start_time = current_time
                        
                        # Process frame if enough time has passed or it's the first frame
                        current_time = time.time()
                        should_process = (
                            frame_num % process_rate == 0 and 
                            current_time - st.session_state.last_process_time >= process_interval
                        )
                        
                        if should_process:
                            st.session_state.last_process_time = current_time
                            
                            # Run face detection in a background thread
                            def process_frame(frame_to_process, frame_number):
                                # Detect faces
                                faces, aligned_faces = face_detector.detect(frame_to_process)
                                
                                # Update face tracker
                                stable_faces = face_tracker.update(faces, frame_number)
                                
                                # Recognize stable faces only - OPTIMIZED to avoid unnecessary processing
                                stable_indices = []
                                stable_names = []
                                stable_scores = []
                                aligned_face_map = {}
                                
                                # Create a lookup map for aligned faces based on approximate position
                                if len(aligned_faces) > 0 and len(faces) > 0:
                                    for i, face_box in enumerate(faces):
                                        if i < len(aligned_faces):
                                            # Use face box center as key
                                            center_x = int(face_box[0] + face_box[2] / 2)
                                            center_y = int(face_box[1] + face_box[3] / 2)
                                            aligned_face_map[(center_x, center_y)] = aligned_faces[i]
                                
                                # Only process stable faces
                                for i, face in enumerate(stable_faces):
                                    if face['is_stable']:
                                        stable_indices.append(i)
                                        box = face['box']
                                        
                                        # Find closest aligned face
                                        center_x = int(box[0] + box[2] / 2)
                                        center_y = int(box[1] + box[3] / 2)
                                        
                                        # Find closest aligned face in map within reasonable distance
                                        best_dist = float('inf')
                                        best_face = None
                                        for (x, y), aligned_face in aligned_face_map.items():
                                            dist = abs(x - center_x) + abs(y - center_y)
                                            if dist < best_dist and dist < 50:  # 50 pixel threshold
                                                best_dist = dist
                                                best_face = aligned_face
                                        
                                        # If found, recognize the face
                                        if best_face is not None:
                                            name, score = face_recognizer.identify(best_face, 0.6)
                                            stable_names.append(name)
                                            stable_scores.append(score)
                                        else:
                                            # If no aligned face found, use Unknown
                                            stable_names.append("Unknown")
                                            stable_scores.append(0.0)
                                
                                # Draw results
                                from utils.face_utils import draw_stable_results
                                processed_frame = draw_stable_results(
                                    frame_to_process, stable_faces, stable_names, stable_scores
                                )
                                
                                # Add FPS counter
                                cv2.putText(
                                    processed_frame, f"FPS: {current_fps}", 
                                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2
                                )
                                
                                return processed_frame, stable_names, stable_scores
                            
                            # Process current frame
                            result_frame, names, scores = process_frame(frame.copy(), frame_num)
                        
                        frame_num += 1
                        
                        # Always display the last processed frame if available
                        display_frame = result_frame if result_frame is not None else frame
                        
                        # Add FPS text to raw frame if no processed frame
                        if result_frame is None:
                            cv2.putText(
                                display_frame, f"FPS: {current_fps}", 
                                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2
                            )
                        
                        # Convert to RGB for display
                        display_frame_rgb = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
                        
                        # Display frame
                        video_placeholder.image(display_frame_rgb, channels="RGB", use_container_width=True)
                        
                        # Update recognition info (only update occasionally to avoid UI slowdown)
                        current_time = time.time()
                        if current_time - last_info_update > 0.5:  # Update every 0.5 seconds
                            last_info_update = current_time
                            
                            # Filter by confidence threshold 0.6
                            high_conf_indices = [i for i, score in enumerate(scores) if score >= 0.6]
                            
                            # Display information based on high confidence detections
                            with info_placeholder:
                                if high_conf_indices:
                                    st.markdown("### Ng∆∞·ªùi ƒë∆∞·ª£c nh·∫≠n d·∫°ng:")
                                    cols = st.columns(min(len(high_conf_indices), 3))
                                    for idx, i in enumerate(high_conf_indices):
                                        with cols[idx % 3]:
                                            status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if names[i] != "Unknown" else "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c"
                                            st.markdown(f"""
                                            **Ng∆∞·ªùi {idx+1}**  
                                            {names[i]}  
                                            Score: {scores[i]:.2f}  
                                            {status}
                                            """)
                                else:
                                    if names:
                                        st.info("Kh√¥ng c√≥ khu√¥n m·∫∑t n√†o ƒë∆∞·ª£c nh·∫≠n d·∫°ng v·ªõi ƒë·ªô tin c·∫≠y > 0.6")
                                    else:
                                        st.markdown("*ƒêang t√¨m ki·∫øm khu√¥n m·∫∑t...*")
                        
                        # Prevent CPU overuse and reduce UI bottlenecks
                        time.sleep(0.01)
                        
                        # Check if stop button was pressed
                        if not st.session_state.video_running:
                            break
                        
                except Exception as e:
                    st.error(f"L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω video: {str(e)}")
                finally:
                    # Release camera
                    if cap is not None:
                        cap.release()
                    video_placeholder.markdown("**Camera ƒë√£ ƒë∆∞·ª£c ƒë√≥ng**")
        
        elif not st.session_state.video_running and not start_button:
            video_placeholder.markdown("""
            ## üé• Ch·∫ø ƒë·ªô Video Tr·ª±c Ti·∫øp
            
            **T√≠nh nƒÉng:**
            - Hi·ªÉn th·ªã khung khu√¥n m·∫∑t ·ªïn ƒë·ªãnh, kh√¥ng ch·ªõp nh√°y
            - T·ª± ƒë·ªông nh·∫≠n d·∫°ng khu√¥n m·∫∑t trong th·ªùi gian th·ª±c
            - Hi·ªÉn th·ªã FPS ƒë·ªÉ theo d√µi hi·ªáu su·∫•t
            - Ch·ªâ hi·ªÉn th·ªã k·∫øt qu·∫£ c√≥ ƒë·ªô tin c·∫≠y > 0.6
            
            **T√πy ch·ªçn hi·ªáu su·∫•t:**
            - ƒê·ªô ph√¢n gi·∫£i th·∫•p = nhanh h∆°n
            - T·ªëc ƒë·ªô x·ª≠ l√Ω cao = x·ª≠ l√Ω √≠t frame h∆°n, m∆∞·ª£t h∆°n
            
            Nh·∫•n **B·∫Øt ƒë·∫ßu** ƒë·ªÉ m·ªü camera.
            """)
    
    elif mode == "üìπ Video upload":
        st.markdown("### üìπ Ph√¢n t√≠ch video t·ª´ file")
        
        # Upload video file
        uploaded_video = st.file_uploader("Ch·ªçn video ƒë·ªÉ ph√¢n t√≠ch", type=['mp4', 'avi', 'mov', 'mkv'])
        
        if uploaded_video is not None:
            # Save uploaded video to temp file
            with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as temp_file:
                temp_file.write(uploaded_video.read())
                video_path = temp_file.name
            
            # Video settings
            st.subheader("C√†i ƒë·∫∑t ph√¢n t√≠ch video")
            col1, col2 = st.columns(2)
            
            with col1:
                sample_rate = st.slider("T·ª∑ l·ªá khung h√¨nh ph√¢n t√≠ch:", 
                                      min_value=1, max_value=30, value=5, 
                                      help="M·ªói bao nhi√™u khung h√¨nh th√¨ ph√¢n t√≠ch m·ªôt l·∫ßn (1 = ph√¢n t√≠ch m·ªçi khung h√¨nh)")
            
            with col2:
                confidence_threshold = st.slider("Ng∆∞·ª°ng ƒë·ªô tin c·∫≠y:", 
                                              min_value=0.5, max_value=1.0, value=0.6, step=0.05,
                                              help="Ch·ªâ hi·ªÉn th·ªã khu√¥n m·∫∑t c√≥ ƒë·ªô tin c·∫≠y t·ª´ ng∆∞·ª°ng n√†y tr·ªü l√™n")
            
            # Create analysis options
            exp_options = st.expander("T√πy ch·ªçn ph√¢n t√≠ch n√¢ng cao", expanded=False)
            with exp_options:
                col1, col2 = st.columns(2)
                with col1:
                    output_fps = st.slider("T·ªëc ƒë·ªô video k·∫øt qu·∫£ (FPS):", 
                                         min_value=5, max_value=30, value=15,
                                         help="S·ªë khung h√¨nh m·ªói gi√¢y trong video k·∫øt qu·∫£")
                
                with col2:
                    max_results = st.slider("S·ªë khu√¥n m·∫∑t t·ªëi ƒëa:", 
                                           min_value=1, max_value=10, value=5,
                                           help="S·ªë l∆∞·ª£ng khu√¥n m·∫∑t t·ªëi ƒëa ph√¢n t√≠ch trong m·ªói khung h√¨nh")
                
                generate_summary = st.checkbox("T·∫°o b√°o c√°o th·ªëng k√™", value=True,
                                             help="T·∫°o b√°o c√°o th·ªëng k√™ c√°c c√° nh√¢n xu·∫•t hi·ªán trong video")
                
                save_output = st.checkbox("L∆∞u video v·ªõi k·∫øt qu·∫£ nh·∫≠n d·∫°ng", value=True,
                                        help="T·∫°o video m·ªõi c√≥ ch·ª©a k·∫øt qu·∫£ nh·∫≠n d·∫°ng khu√¥n m·∫∑t")
            
            # Initialize face tracker
            face_tracker = load_face_tracker()
            
            # Analysis button
            analyze_button = st.button("üìä B·∫Øt ƒë·∫ßu ph√¢n t√≠ch", type="primary", use_container_width=True)
            
            if analyze_button:
                # Check if video can be opened
                cap = cv2.VideoCapture(video_path)
                if not cap.isOpened():
                    st.error("Kh√¥ng th·ªÉ m·ªü video. Vui l√≤ng ki·ªÉm tra l·∫°i file video.")
                    try:
                        os.unlink(video_path)  # Clean up temp file
                    except:
                        pass
                else:
                    # Video information
                    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                    original_fps = cap.get(cv2.CAP_PROP_FPS)
                    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                    duration = total_frames / original_fps if original_fps > 0 else 0
                    
                    # Display video info
                    st.info(f"Th√¥ng tin video: {video_width}x{video_height}, {original_fps:.1f} FPS, {duration:.1f} gi√¢y, {total_frames} khung h√¨nh")
                    
                    # Setup progress display and storage
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    video_preview = st.empty()
                    
                    # Setup output video writer if needed
                    output_video_path = None
                    video_writer = None
                    if save_output:
                        output_video_path = video_path.replace('.mp4', '_analyzed.mp4')
                        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                        video_writer = cv2.VideoWriter(
                            output_video_path, 
                            fourcc, 
                            output_fps,
                            (video_width, video_height)
                        )
                    
                    # Statistics storage
                    person_appearances = {}  # Store person name -> number of appearances
                    person_first_seen = {}   # Store when the person first appears
                    person_timings = {}      # Store timestamp ranges for each person
                    frame_results = []       # Store detection results for frames
                    
                    # Process video
                    with st.spinner("ƒêang x·ª≠ l√Ω video..."):
                        frame_count = 0
                        processed_count = 0
                        
                        while True:
                            ret, frame = cap.read()
                            if not ret:
                                break
                            
                            # Process only every sample_rate frames to save time
                            if frame_count % sample_rate == 0:
                                # Current timestamp
                                timestamp = frame_count / original_fps
                                timestamp_str = f"{int(timestamp // 60):02d}:{int(timestamp % 60):02d}"
                                
                                # Update progress 
                                progress = frame_count / total_frames
                                progress_bar.progress(progress)
                                status_text.text(f"ƒêang x·ª≠ l√Ω: {frame_count}/{total_frames} khung h√¨nh ({progress*100:.1f}%) - {timestamp_str}")
                                
                                # Detect faces in frame
                                faces, aligned_faces = face_detector.detect(frame)
                                
                                # Update face tracker
                                stable_faces = face_tracker.update(faces, frame_count)
                                
                                # Process only stable faces
                                if len(aligned_faces) > 0 and len(faces) > 0:
                                    # Create a mapping between detected faces and aligned faces
                                    aligned_face_map = {}
                                    for i, face_box in enumerate(faces):
                                        if i < len(aligned_faces):
                                            center_x = int(face_box[0] + face_box[2] / 2)
                                            center_y = int(face_box[1] + face_box[3] / 2)
                                            aligned_face_map[(center_x, center_y)] = aligned_faces[i]
                                    
                                    # Process stable faces
                                    names = []
                                    scores = []
                                    
                                    for face in stable_faces:
                                        if face['is_stable']:
                                            box = face['box']
                                            
                                            # Find closest aligned face
                                            center_x = int(box[0] + box[2] / 2)
                                            center_y = int(box[1] + box[3] / 2)
                                            
                                            # Find closest aligned face within threshold
                                            best_dist = float('inf')
                                            best_face = None
                                            for (x, y), aligned_face in aligned_face_map.items():
                                                dist = abs(x - center_x) + abs(y - center_y)
                                                if dist < best_dist and dist < 50:
                                                    best_dist = dist
                                                    best_face = aligned_face
                                            
                                            # If found, recognize face
                                            if best_face is not None:
                                                name, score = face_recognizer.identify(best_face, confidence_threshold)
                                                names.append(name)
                                                scores.append(score)
                                            else:
                                                names.append("Unknown")
                                                scores.append(0.0)
                                    
                                    # Draw results
                                    from utils.face_utils import draw_stable_results
                                    result_frame = draw_stable_results(frame, stable_faces, names, scores)
                                    
                                    # Add timestamp to frame
                                    cv2.putText(
                                        result_frame, timestamp_str, 
                                        (video_width - 100, 30), cv2.FONT_HERSHEY_SIMPLEX, 
                                        0.7, (255, 255, 255), 2
                                    )
                                    
                                    # Update statistics
                                    for i, name in enumerate(names):
                                        if i < len(scores) and scores[i] >= confidence_threshold:
                                            if name != "Unknown":
                                                # Update appearance count
                                                if name in person_appearances:
                                                    person_appearances[name] += 1
                                                else:
                                                    person_appearances[name] = 1
                                                    person_first_seen[name] = timestamp
                                                
                                                # Update timing ranges
                                                if name not in person_timings:
                                                    person_timings[name] = []
                                                
                                                # Check if this is a new timing segment or continuation
                                                if not person_timings[name] or timestamp - person_timings[name][-1][1] > 3.0:
                                                    # New segment (gap > 3 seconds)
                                                    person_timings[name].append([timestamp, timestamp])
                                                else:
                                                    # Update end time of the last segment
                                                    person_timings[name][-1][1] = timestamp
                                    
                                    # Store frame result for summary
                                    frame_results.append({
                                        'frame': frame_count,
                                        'timestamp': timestamp,
                                        'names': [n for i, n in enumerate(names) if i < len(scores) and scores[i] >= confidence_threshold],
                                        'scores': [s for s in scores if s >= confidence_threshold]
                                    })
                                else:
                                    # No faces detected, use original frame
                                    result_frame = frame.copy()
                                    cv2.putText(
                                        result_frame, timestamp_str, 
                                        (video_width - 100, 30), cv2.FONT_HERSHEY_SIMPLEX, 
                                        0.7, (255, 255, 255), 2
                                    )
                                
                                # Write to output video if enabled
                                if video_writer is not None:
                                    video_writer.write(result_frame)
                                
                                # Show preview occasionally
                                if processed_count % 10 == 0:
                                    video_preview.image(
                                        cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB),
                                        caption=f"Khung h√¨nh {frame_count} - {timestamp_str}",
                                        use_container_width=True
                                    )
                                
                                processed_count += 1
                            
                            frame_count += 1
                            
                            # Check for stop button
                            if st.button("D·ª´ng ph√¢n t√≠ch", key="stop_video_analysis"):
                                break
                        
                        # Complete progress bar
                        progress_bar.progress(1.0)
                        status_text.text(f"Ho√†n th√†nh ph√¢n t√≠ch video: {processed_count} khung h√¨nh ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω")
                    
                    # Release resources
                    cap.release()
                    if video_writer is not None:
                        video_writer.release()
                    
                    # Display results
                    if generate_summary and person_appearances:
                        st.subheader("üìä K·∫øt qu·∫£ ph√¢n t√≠ch")
                        
                        # Display summary of people detected
                        st.markdown("#### Danh s√°ch ng∆∞·ªùi ƒë∆∞·ª£c nh·∫≠n d·∫°ng")
                        summary_data = []
                        for name, count in sorted(person_appearances.items(), key=lambda x: x[1], reverse=True):
                            # Calculate percentage of video
                            percentage = (count * sample_rate / total_frames) * 100
                            
                            # Calculate time ranges string
                            time_ranges = []
                            for start, end in person_timings.get(name, []):
                                start_str = f"{int(start // 60):02d}:{int(start % 60):02d}"
                                end_str = f"{int(end // 60):02d}:{int(end % 60):02d}"
                                time_ranges.append(f"{start_str}-{end_str}")
                            
                            time_ranges_str = ", ".join(time_ranges[:3])
                            if len(time_ranges) > 3:
                                time_ranges_str += f" v√† {len(time_ranges) - 3} kho·∫£ng th·ªùi gian kh√°c"
                            
                            # First seen
                            first_seen = person_first_seen.get(name, 0)
                            first_seen_str = f"{int(first_seen // 60):02d}:{int(first_seen % 60):02d}"
                            
                            summary_data.append({
                                "H·ªç t√™n": name,
                                "S·ªë khung h√¨nh xu·∫•t hi·ªán": count,
                                "T·ª∑ l·ªá xu·∫•t hi·ªán": f"{percentage:.1f}%",
                                "Xu·∫•t hi·ªán l·∫ßn ƒë·∫ßu": first_seen_str,
                                "C√°c kho·∫£ng th·ªùi gian": time_ranges_str
                            })
                        
                        # Display as table
                        st.table(summary_data)
                        
                        # Make a time-based visualization of appearances
                        st.markdown("#### Bi·ªÉu ƒë·ªì th·ªùi gian xu·∫•t hi·ªán")
                        
                        # Create horizontal timeline visualization
                        timeline_height = 80
                        person_height = 20
                        padding = 5
                        text_width = 150
                        
                        # Calculate timeline width based on duration
                        seconds_width = 4  # pixels per second
                        timeline_width = int(max(800, duration * seconds_width))
                        
                        # Create timeline image
                        timeline_image = np.ones((len(person_timings) * (person_height + padding) + padding, 
                                                timeline_width + text_width, 3), dtype=np.uint8) * 255
                        
                        # Draw timeline for each person
                        for i, (name, time_ranges) in enumerate(person_timings.items()):
                            # Calculate y position
                            y_pos = padding + i * (person_height + padding)
                            
                            # Draw name
                            cv2.putText(
                                timeline_image, name, 
                                (5, y_pos + person_height - 5), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1
                            )
                            
                            # Draw timeline base
                            cv2.line(
                                timeline_image,
                                (text_width, y_pos + person_height // 2),
                                (text_width + timeline_width, y_pos + person_height // 2),
                                (200, 200, 200), 1
                            )
                            
                            # Draw appearance blocks
                            for start, end in time_ranges:
                                start_pos = text_width + int(start * seconds_width)
                                end_pos = text_width + int(end * seconds_width)
                                
                                cv2.rectangle(
                                    timeline_image,
                                    (start_pos, y_pos),
                                    (end_pos, y_pos + person_height),
                                    (0, 150, 0), -1
                                )
                            
                        # Draw time markers
                        marker_interval = 60  # 1 minute
                        for t in range(0, int(duration) + marker_interval, marker_interval):
                            x_pos = text_width + int(t * seconds_width)
                            
                            # Draw vertical line
                            cv2.line(
                                timeline_image,
                                (x_pos, 0),
                                (x_pos, timeline_image.shape[0]),
                                (180, 180, 180), 1
                            )
                            
                            # Draw time label
                            time_label = f"{t//60}:{t%60:02d}"
                            cv2.putText(
                                timeline_image, time_label,
                                (x_pos - 20, timeline_image.shape[0] - 5),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1
                            )
                        
                        # Display timeline
                        st.image(timeline_image, use_container_width=True)
                    
                    # If video was saved, provide download link
                    if save_output and output_video_path and os.path.exists(output_video_path):
                        # Read the video file
                        with open(output_video_path, 'rb') as file:
                            video_bytes = file.read()
                        
                        # Create download button
                        st.download_button(
                            label="‚¨áÔ∏è T·∫£i xu·ªëng video k·∫øt qu·∫£",
                            data=video_bytes,
                            file_name=f"video_analyzed.mp4",
                            mime="video/mp4"
                        )
                    
                    # Clean up temporary files
                    try:
                        if os.path.exists(video_path):
                            os.unlink(video_path)
                        if output_video_path and os.path.exists(output_video_path):
                            # Don't delete right away, as it might be used for download
                            pass
                    except:
                        pass
                    
                    st.success("‚úÖ Ph√¢n t√≠ch video ho√†n t·∫•t!")