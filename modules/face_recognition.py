import streamlit as st
import cv2
import numpy as np
import time
import os
from typing import List, Dict, Tuple, Optional

@st.cache_resource
def load_face_detector():
    from utils.face_utils import FaceDetector
    return FaceDetector("models/face_detection_yunet_2023mar.onnx")

@st.cache_resource
def load_face_recognizer():
    from utils.face_utils import FaceRecognizer
    return FaceRecognizer("models/face_recognition_sface_2021dec.onnx", "data/db_embeddings.pkl")

class StableFaceTracker:
    def __init__(self, max_distance: float = 50.0, min_frames: int = 5, max_missing_frames: int = 15):
        self.tracked_faces: List[Dict] = []
        self.max_distance = max_distance
        self.min_frames = min_frames
        self.max_missing_frames = max_missing_frames
        
    def update(self, detected_faces: np.ndarray, frame_num: int) -> List[Dict]:
        """Update tracked faces with new detections"""
        # If no detections, gradually remove tracked faces
        if len(detected_faces) == 0:
            # Remove faces that have been missing for too long
            self.tracked_faces = [face for face in self.tracked_faces 
                                if frame_num - face['last_seen'] < self.max_missing_frames]
            
            # Gradually decrease confidence for missing faces
            for face in self.tracked_faces:
                face['score'] *= 0.95  # Decay score each frame
                face['frame_count'] = max(0, face['frame_count'] - 1)
                if face['frame_count'] < self.min_frames:
                    face['is_stable'] = False
            
            return self.tracked_faces
        
        # Track existing faces first
        if len(self.tracked_faces) > 0:
            # Mark all as unmatched initially
            unmatched_tracked = list(range(len(self.tracked_faces)))
            matched_new = set()
            
            # Find matches between new and tracked faces
            for j, new_face in enumerate(detected_faces):
                best_match_idx = None
                min_dist = float('inf')
                
                for i in unmatched_tracked:
                    tracked = self.tracked_faces[i]
                    dist = self._calculate_distance(new_face[:4], tracked['box'])
                    if dist < min_dist and dist < self.max_distance:
                        min_dist = dist
                        best_match_idx = i
                
                if best_match_idx is not None:
                    # Update existing face
                    tracked = self.tracked_faces[best_match_idx]
                    tracked['box'] = self._smooth_box(tracked['box'], new_face[:4])
                    tracked['score'] = 0.9 * tracked['score'] + 0.1 * new_face[4]
                    tracked['frame_count'] += 1
                    tracked['last_seen'] = frame_num
                    
                    # Mark as stable if seen for enough frames
                    if tracked['frame_count'] >= self.min_frames:
                        tracked['is_stable'] = True
                    
                    unmatched_tracked.remove(best_match_idx)
                    matched_new.add(j)
            
            # Add new faces
            for j, new_face in enumerate(detected_faces):
                if j not in matched_new:
                    self.tracked_faces.append({
                        'box': new_face[:4],
                        'score': new_face[4],
                        'frame_count': 1,
                        'last_seen': frame_num,
                        'is_stable': False
                    })
        else:
            # Add all detected faces as new
            for new_face in detected_faces:
                self.tracked_faces.append({
                    'box': new_face[:4],
                    'score': new_face[4],
                    'frame_count': 1,
                    'last_seen': frame_num,
                    'is_stable': False
                })
        
        # Remove old faces
        self.tracked_faces = [face for face in self.tracked_faces 
                            if frame_num - face['last_seen'] < self.max_missing_frames]
        
        return self.tracked_faces
    
    def _calculate_distance(self, box1: np.ndarray, box2: np.ndarray) -> float:
        """Calculate distance between two face boxes"""
        x1, y1, w1, h1 = box1
        x2, y2, w2, h2 = box2
        center1 = (x1 + w1/2, y1 + h1/2)
        center2 = (x2 + w2/2, y2 + h2/2)
        return np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)
    
    def _smooth_box(self, old_box: np.ndarray, new_box: np.ndarray, alpha: float = 0.3) -> np.ndarray:
        """Smooth transition between box positions"""
        return old_box * (1 - alpha) + new_box * alpha

def draw_stable_results(frame: np.ndarray, stable_faces: List[Dict], 
                       names: List[str] = None, scores: List[float] = None) -> np.ndarray:
    """Draw detection and recognition results on the frame with stability (OPTIMIZED)"""
    result = frame.copy()
    
    # Pre-calculate font settings
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.7
    thickness = 2
    
    # Draw only stable faces
    stable_index = 0
    for face in stable_faces:
        if not face['is_stable']:
            continue
            
        x, y, w, h = map(int, face['box'])
        
        # Get name and score if available
        name = names[stable_index] if names and stable_index < len(names) else None
        score = scores[stable_index] if scores and stable_index < len(scores) else None
        
        # Choose color based on recognition status
        color = (0, 255, 0)  # Green for both detected and recognized faces
        text_color = (255, 255, 255)  # White text
        
        # Draw face rectangle (simpler draw method)
        cv2.rectangle(result, (x, y), (x+w, y+h), color, 2)
        
        # Draw label if available
        if name is not None:
            label = f"{name} ({score:.2f})" if score is not None else name
            
            # Get text size once
            (text_width, text_height), baseline = cv2.getTextSize(label, font, font_scale, thickness)
            
            # Calculate text position
            text_y = y - 10 if y - 10 > 10 else y + h + 20
            
            # Draw text background (simpler rectangle)
            cv2.rectangle(result, 
                         (x, text_y - text_height - baseline), 
                         (x + text_width + 5, text_y + baseline),
                         color, cv2.FILLED)
            
            # Draw text
            cv2.putText(result, label, (x + 2, text_y - 5),
                        font, font_scale, text_color, thickness)
        
        stable_index += 1
    
    return result

def show():
    # Th√™m ph·∫ßn gi·ªõi thi·ªáu v√† h∆∞·ªõng d·∫´n
    with st.expander("üîç Gi·ªõi thi·ªáu v·ªÅ nh·∫≠n d·∫°ng khu√¥n m·∫∑t", expanded=False):
        st.markdown("""
        ### Gi·ªõi thi·ªáu v·ªÅ nh·∫≠n d·∫°ng khu√¥n m·∫∑t
        
        T√≠nh nƒÉng nh·∫≠n d·∫°ng khu√¥n m·∫∑t trong ·ª©ng d·ª•ng n√†y s·ª≠ d·ª•ng ki·∫øn tr√∫c hai giai ƒëo·∫°n hi·ªán ƒë·∫°i:
        
        1. **YuNet Face Detector**: M√¥ h√¨nh ph√°t hi·ªán khu√¥n m·∫∑t d·ª±a tr√™n CNN (Convolutional Neural Network) ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a
           - Ki·∫øn tr√∫c: Lightweight network v·ªõi c√°c kh·ªëi ResNet c·∫£i ti·∫øn
           - Hi·ªáu su·∫•t: ƒê·ªô ch√≠nh x√°c cao v√† x·ª≠ l√Ω th·ªùi gian th·ª±c tr√™n CPU
           - ƒê·ªãnh d·∫°ng: ONNX v·ªõi k√≠ch th∆∞·ªõc nh·ªè (< 1MB)
        
        2. **SFace Recognition**: M√¥ h√¨nh nh·∫≠n d·∫°ng khu√¥n m·∫∑t v·ªõi ƒë·ªô ch√≠nh x√°c cao
           - Ki·∫øn tr√∫c: D·ª±a tr√™n m·∫°ng ResNet-50 c·∫£i ti·∫øn cho c√°c embedding vector 128 chi·ªÅu
           - Training: ƒê∆∞·ª£c hu·∫•n luy·ªán tr√™n dataset l·ªõn v·ªõi loss function ƒë·∫∑c bi·ªát gi√∫p ph√¢n bi·ªát t·ªët c√°c khu√¥n m·∫∑t
           - Ph∆∞∆°ng ph√°p: So s√°nh vector embedding v·ªõi c√°c vector trong c∆° s·ªü d·ªØ li·ªáu
        
        ·ª®ng d·ª•ng c≈©ng t√≠ch h·ª£p thu·∫≠t to√°n theo d√µi khu√¥n m·∫∑t (face tracking) ƒë·ªÉ ƒë·∫£m b·∫£o ·ªïn ƒë·ªãnh khi nh·∫≠n d·∫°ng video th·ªùi gian th·ª±c, gi·∫£m hi·ªán t∆∞·ª£ng "ch·ªõp nh√°y" khi x√°c ƒë·ªãnh danh t√≠nh.
        
        **C√°c t√≠nh nƒÉng ch√≠nh:**
        - Nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ ·∫£nh tƒ©nh (t·∫£i l√™n ho·∫∑c ch·ª•p t·ª´ webcam)
        - Nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ video t·∫£i l√™n (nhi·ªÅu ƒë·ªãnh d·∫°ng nh∆∞ MP4, AVI, MOV, v.v.)
        - Ph√°t hi·ªán v√† nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ webcam theo th·ªùi gian th·ª±c
        - Theo d√µi khu√¥n m·∫∑t ·ªïn ƒë·ªãnh gi√∫p gi·∫£m ƒë√°ng k·ªÉ sai s·ªë trong nh·∫≠n d·∫°ng
        
        **·ª®ng d·ª•ng th·ª±c t·∫ø:**
        - H·ªá th·ªëng b·∫£o m·∫≠t v√† ki·ªÉm so√°t truy c·∫≠p
        - H·ªá th·ªëng ƒëi·ªÉm danh t·ª± ƒë·ªông
        - Ph√¢n t√≠ch video gi√°m s√°t
        - Nh·∫≠n d·∫°ng khu√¥n m·∫∑t trong d·ªØ li·ªáu h√¨nh ·∫£nh v√† video l·ªõn
        - Tr·∫£i nghi·ªám c√° nh√¢n h√≥a trong c√°c h·ªá th·ªëng th√¥ng minh
        - X√°c th·ª±c danh t√≠nh kh√¥ng ti·∫øp x√∫c
        """)
            
    with st.expander("üìã H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng", expanded=False):
        st.markdown("""
        ### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng
        
        #### 1. Ch·∫ø ƒë·ªô ·∫£nh tƒ©nh
        - **Upload ·∫£nh**: T·∫£i l√™n ·∫£nh ch·ª©a khu√¥n m·∫∑t c·∫ßn nh·∫≠n d·∫°ng
        - **Ch·ª•p t·ª´ webcam**: Ch·ª•p ·∫£nh tr·ª±c ti·∫øp t·ª´ webcam ƒë·ªÉ nh·∫≠n d·∫°ng
        
        #### 2. Ch·∫ø ƒë·ªô video t·∫£i l√™n
        - **Upload video**: T·∫£i l√™n video c√≥ ƒë·ªãnh d·∫°ng MP4, AVI, MOV, MKV, v.v.
        - **ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô x·ª≠ l√Ω**: Ch·ªçn t·ªëc ƒë·ªô x·ª≠ l√Ω (Ch·∫ø ƒë·ªô nhanh / Ch·∫ø ƒë·ªô ch·∫•t l∆∞·ª£ng cao)
        - **T√πy ch·ªçn hi·ªÉn th·ªã**: Hi·ªÉn th·ªã k·∫øt qu·∫£ theo t·ª´ng frame ho·∫∑c video ho√†n ch·ªânh
        - **Thanh ƒëi·ªÅu khi·ªÉn video**: T·∫°m d·ª´ng, tua ƒëi, tua l·∫°i, v.v.
        
        #### 3. Ch·∫ø ƒë·ªô video tr·ª±c ti·∫øp
        - **B·∫Øt ƒë·∫ßu**: M·ªü camera v√† b·∫Øt ƒë·∫ßu ph√°t hi·ªán khu√¥n m·∫∑t
        - **D·ª´ng**: D·ª´ng qu√° tr√¨nh nh·∫≠n d·∫°ng v√† ƒë√≥ng camera
        - **ƒêi·ªÅu ch·ªânh ƒë·ªô ph√¢n gi·∫£i**: Ch·ªçn ƒë·ªô ph√¢n gi·∫£i camera ph√π h·ª£p
        - **T·ªëc ƒë·ªô x·ª≠ l√Ω**: ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô x·ª≠ l√Ω khung h√¨nh (gi√° tr·ªã th·∫•p h∆°n = x·ª≠ l√Ω nhi·ªÅu frame h∆°n)
        
        #### M·∫πo s·ª≠ d·ª•ng:
        - **√Ånh s√°ng**: ƒê·∫£m b·∫£o khu√¥n m·∫∑t ƒë∆∞·ª£c chi·∫øu s√°ng t·ªët
        - **G√≥c nh√¨n**: N√™n ch·ªçn g√≥c th·∫≥ng ho·∫∑c nghi√™ng nh·∫π ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët nh·∫•t
        - **Kho·∫£ng c√°ch**: Khu√¥n m·∫∑t n√™n chi·∫øm kho·∫£ng 10-15% khung h√¨nh ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªëi ∆∞u
        - **Ch·∫•t l∆∞·ª£ng video**: ∆Øu ti√™n video c√≥ ƒë·ªô ph√¢n gi·∫£i cao (720p tr·ªü l√™n) v√† √≠t nhi·ªÖu
        - **ƒêƒÉng k√Ω m·∫∑t m·ªõi**: N·∫øu khu√¥n m·∫∑t ch∆∞a ƒë∆∞·ª£c nh·∫≠n d·∫°ng, s·ª≠ d·ª•ng ch·ª©c nƒÉng "ƒêƒÉng k√Ω khu√¥n m·∫∑t m·ªõi"
        
        #### X·ª≠ l√Ω l·ªói:
        - **Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c**: ƒê·∫£m b·∫£o khu√¥n m·∫∑t ƒë√£ ƒë∆∞·ª£c ƒëƒÉng k√Ω trong h·ªá th·ªëng
        - **Nh·∫≠n d·∫°ng sai**: C·∫≠p nh·∫≠t database v·ªõi nhi·ªÅu m·∫´u khu√¥n m·∫∑t h∆°n
        - **L·ªói x·ª≠ l√Ω video**: Th·ª≠ chuy·ªÉn ƒë·ªïi video sang MP4 ho·∫∑c gi·∫£m ƒë·ªô ph√¢n gi·∫£i
        - **Kh√¥ng hi·ªÉn th·ªã camera**: Ki·ªÉm tra quy·ªÅn truy c·∫≠p camera trong tr√¨nh duy·ªát
        """)
    
    # Load models
    face_detector = load_face_detector()
    face_recognizer = load_face_recognizer()
    
    # Choose input mode
    mode = st.radio("Ch·ªçn ch·∫ø ƒë·ªô nh·∫≠n d·∫°ng:", ["üì∏ ·∫¢nh tƒ©nh", "üé¨ Video t·∫£i l√™n", "üé• Video tr·ª±c ti·∫øp"])
    
    # Process based on selected mode
    if mode == "üì∏ ·∫¢nh tƒ©nh":
        # Image input methods
        input_method = st.radio("Ch·ªçn ngu·ªìn ·∫£nh:", ["Upload ·∫£nh", "Ch·ª•p t·ª´ webcam"])
        
        if input_method == "Ch·ª•p t·ª´ webcam":
            # Camera capture interface  
            picture = st.camera_input("Ch·ª•p ·∫£nh khu√¥n m·∫∑t")
            
            if picture:
                # Convert to OpenCV format
                bytes_data = picture.getvalue()
                img = cv2.imdecode(np.frombuffer(bytes_data, np.uint8), cv2.IMREAD_COLOR)
                
                # Create two columns for original and result images
                col1, col2 = st.columns(2)
                
                # Display original image
                with col1:
                    st.subheader("·∫¢nh g·ªëc")
                    st.image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), use_container_width=True)
                
                # Process image
                with st.spinner("ƒêang nh·∫≠n d·∫°ng khu√¥n m·∫∑t..."):
                    faces, aligned_faces = face_detector.detect(img)
                    
                    if len(faces) > 0:
                        names = []
                        scores = []
                        
                        # Recognize faces (limit to 5)
                        for face_img in aligned_faces[:5]:
                            name, score = face_recognizer.identify(face_img)
                            names.append(name)
                            scores.append(score)
                        
                        # Draw results
                        from utils.face_utils import draw_results
                        result_img = draw_results(img, faces[:5], names, scores)
                    else:
                        result_img = img  # No faces detected
                        names = []
                        scores = []
                    
                    # Display result
                    with col2:
                        st.subheader("K·∫øt qu·∫£")
                        st.image(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB), use_container_width=True)
                    
                    # Display information below images
                    if names:
                        st.markdown("### Th√¥ng tin nh·∫≠n d·∫°ng:")
                        for i, (name, score) in enumerate(zip(names, scores)):
                            status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if name != "Unknown" else "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c"
                            st.write(f"**Ng∆∞·ªùi {i+1}:** {name} ({score:.2f}) - {status}")
                    else:
                        st.warning("Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t n√†o trong ·∫£nh!")
        
        else:  # Upload image mode
            uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh ch·ª©a khu√¥n m·∫∑t", type=["jpg", "jpeg", "png"])
            
            if uploaded_file is not None:
                # Read image
                file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
                img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
                
                # Create two columns for original and result images
                col1, col2 = st.columns(2)
                
                # Display original image
                with col1:
                    st.subheader("·∫¢nh g·ªëc")
                    st.image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), use_container_width=True)
                
                # Process image
                with st.spinner("ƒêang nh·∫≠n d·∫°ng khu√¥n m·∫∑t..."):
                    faces, aligned_faces = face_detector.detect(img)
                    
                    if len(faces) > 0:
                        names = []
                        scores = []
                        
                        # Recognize faces (limit to 5)
                        for face_img in aligned_faces[:5]:
                            name, score = face_recognizer.identify(face_img)
                            names.append(name)
                            scores.append(score)
                        
                        # Draw results
                        from utils.face_utils import draw_results
                        result_img = draw_results(img, faces[:5], names, scores)
                    else:
                        result_img = img  # No faces detected
                        names = []
                        scores = []
                    
                    # Display result
                    with col2:
                        st.subheader("K·∫øt qu·∫£")
                        st.image(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB), use_container_width=True)
                    
                    # Display information below images
                    if names:
                        st.markdown("### Th√¥ng tin nh·∫≠n d·∫°ng:")
                        for i, (name, score) in enumerate(zip(names, scores)):
                            status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if name != "Unknown" else "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c"
                            st.write(f"**Ng∆∞·ªùi {i+1}:** {name} ({score:.2f}) - {status}")
                    else:
                        st.warning("Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t n√†o trong ·∫£nh!")
    
    elif mode == "üé¨ Video t·∫£i l√™n":
        # Video upload section
        st.subheader("Nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ video")
        
        # Upload video file
        video_file = st.file_uploader("T·∫£i l√™n video", type=["mp4", "mov", "avi", "mkv", "wmv"])
        
        if video_file is not None:
            # Save uploaded video to a temporary file
            temp_video_path = f"temp_video_{int(time.time())}.mp4"
            with open(temp_video_path, "wb") as f:
                f.write(video_file.getbuffer())
                
            # Get video info
            cap = cv2.VideoCapture(temp_video_path)
            if not cap.isOpened():
                st.error("Kh√¥ng th·ªÉ ƒë·ªçc video. Vui l√≤ng th·ª≠ l·∫°i v·ªõi ƒë·ªãnh d·∫°ng kh√°c.")
            else:
                # Get video properties
                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                fps = cap.get(cv2.CAP_PROP_FPS)
                
                # Display video info
                st.info(f"Th√¥ng tin video: {frame_width}x{frame_height}, {fps:.1f} FPS, {frame_count} frames")
                
                # Video processing options
                col1, col2 = st.columns(2)
                with col1:
                    process_mode = st.radio(
                        "Ch·∫ø ƒë·ªô x·ª≠ l√Ω:",
                        ["Nhanh (Skip frames)", "Ch·∫•t l∆∞·ª£ng cao (Full frames)"],
                        index=0
                    )
                with col2:
                    result_mode = st.radio(
                        "Hi·ªÉn th·ªã k·∫øt qu·∫£:",
                        ["T·ª´ng frame", "Video ho√†n ch·ªânh"],
                        index=1
                    )
                
                # Skip frames for faster processing
                if process_mode == "Nhanh (Skip frames)":
                    # For long videos, skip more frames
                    if frame_count > 1000:
                        frame_skip = 10
                    elif frame_count > 500:
                        frame_skip = 5
                    else:
                        frame_skip = 2
                else:
                    frame_skip = 1
                
                # Initialize face tracker for more stable results
                face_tracker = StableFaceTracker(max_distance=40.0, min_frames=2, max_missing_frames=8)
                
                # Process video button
                if st.button("B·∫Øt ƒë·∫ßu x·ª≠ l√Ω video", type="primary", use_container_width=True):
                    # Create progress bar and status containers
                    progress_container = st.container()
                    with progress_container:
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        time_remaining = st.empty()
                    
                    # Container for results
                    result_container = st.empty()
                    info_container = st.empty()
                    
                    # If full video output is selected, prepare output video
                    if result_mode == "Video ho√†n ch·ªânh":
                        # Create temporary output video file
                        output_video_path = f"temp_output_{int(time.time())}.mp4"
                        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                        out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))
                    
                    # Process video frames
                    frame_idx = 0
                    processed_frames = 0
                    all_results = []  # List to store [frame_idx, faces, names, scores]
                    
                    # Tracking variables for stats
                    total_faces_detected = 0
                    identified_faces = 0
                    unknown_faces = 0
                    people_detected = set()
                    people_frame_counts = {}  # Track how many frames each person appears in
                    
                    # Time tracking for ETA calculation
                    start_time = time.time()
                    frames_processed_for_time = 0
                    processing_fps = 0
                    
                    try:
                        while True:
                            ret, frame = cap.read()
                            if not ret:
                                break
                                
                            # Process every Nth frame
                            if frame_idx % frame_skip == 0:
                                # Calculate ETA based on current processing speed
                                frames_processed_for_time += 1
                                elapsed_time = time.time() - start_time
                                if elapsed_time > 0:
                                    processing_fps = frames_processed_for_time / elapsed_time
                                    remaining_frames = frame_count - frame_idx
                                    eta_seconds = remaining_frames / processing_fps if processing_fps > 0 else 0
                                    eta_text = f"ETA: {eta_seconds:.0f}s" if eta_seconds < 60 else f"ETA: {eta_seconds/60:.1f}m"
                                else:
                                    eta_text = "Calculating ETA..."
                                
                                # Update progress
                                progress = (frame_idx + 1) / frame_count
                                progress_bar.progress(progress)
                                status_text.text(f"ƒêang x·ª≠ l√Ω: {frame_idx + 1}/{frame_count} frames ({progress*100:.1f}%)")
                                time_remaining.text(f"{eta_text} ‚Äî {processing_fps:.1f} FPS")
                                
                                # Face detection
                                faces, aligned_faces = face_detector.detect(frame)
                                
                                # Update face tracking
                                stable_faces = face_tracker.update(faces, frame_idx)
                                
                                # Identify stable faces
                                stable_names = []
                                stable_scores = []
                                
                                # Only process aligned faces if any were detected
                                if len(faces) > 0:
                                    total_faces_detected += len(faces)
                                    
                                    for face in stable_faces:
                                        if face['is_stable']:
                                            # Find corresponding aligned face (optimized matching)
                                            best_face_img = None
                                            min_dist = float('inf')
                                            
                                            for i, detected_face in enumerate(faces):
                                                # Simplified distance calculation
                                                if i < len(aligned_faces):
                                                    dist = np.sum(np.abs(detected_face[:4] - face['box']))
                                                    if dist < min_dist:
                                                        min_dist = dist
                                                        best_face_img = aligned_faces[i]
                                            
                                            # Identify face if found
                                            if best_face_img is not None:
                                                name, score = face_recognizer.identify(best_face_img)
                                                stable_names.append(name)
                                                stable_scores.append(score)
                                                
                                                # Update stats
                                                if name != "Unknown":
                                                    identified_faces += 1
                                                    people_detected.add(name)
                                                    # Count appearances
                                                    if name in people_frame_counts:
                                                        people_frame_counts[name] += 1
                                                    else:
                                                        people_frame_counts[name] = 1
                                                else:
                                                    unknown_faces += 1
                                
                                # Draw results
                                result_frame = draw_stable_results(frame, stable_faces, stable_names, stable_scores)
                                
                                # Add frame number for reference
                                cv2.putText(
                                    result_frame, 
                                    f"Frame: {frame_idx}/{frame_count}", 
                                    (10, frame_height - 10),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1
                                )
                                
                                # Store results
                                all_results.append([frame_idx, stable_faces, stable_names, stable_scores, result_frame])
                                
                                # Display result if in frame mode
                                if result_mode == "T·ª´ng frame" and processed_frames % 5 == 0:  # Only update display every 5 processed frames
                                    result_container.image(cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB), use_container_width=True)
                                    
                                    # Display face info
                                    if stable_names:
                                        info_text = "**Khu√¥n m·∫∑t ƒë√£ ph√°t hi·ªán:**\n\n"
                                        for name, score in zip(stable_names, stable_scores):
                                            status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if name != "Unknown" else "‚ùå Ch∆∞a nh·∫≠n d·∫°ng"
                                            info_text += f"- {name} ({score:.2f}) - {status}\n"
                                        info_container.markdown(info_text)
                                
                                # Save to output video if in video mode
                                if result_mode == "Video ho√†n ch·ªânh":
                                    out.write(result_frame)
                                
                                processed_frames += 1
                                
                            frame_idx += 1
                    
                    except Exception as e:
                        st.error(f"L·ªói khi x·ª≠ l√Ω video: {str(e)}")
                    finally:
                        # Close video capture and writer
                        cap.release()
                        if result_mode == "Video ho√†n ch·ªânh":
                            out.release()
                        
                        # Clear progress indicators
                        progress_container.empty()
                        
                        # Calculate processing stats
                        total_time = time.time() - start_time
                        processing_fps = processed_frames / total_time if total_time > 0 else 0
                        
                        # Display stats
                        st.success(f"‚úÖ ƒê√£ x·ª≠ l√Ω xong {processed_frames} frames trong {total_time:.1f} gi√¢y ({processing_fps:.1f} FPS)")
                        
                        stats_col1, stats_col2 = st.columns(2)
                        with stats_col1:
                            st.markdown("### Th·ªëng k√™ nh·∫≠n d·∫°ng")
                            st.markdown(f"- T·ªïng s·ªë khu√¥n m·∫∑t ƒë√£ ph√°t hi·ªán: **{total_faces_detected}**")
                            st.markdown(f"- Khu√¥n m·∫∑t ƒë√£ nh·∫≠n d·∫°ng: **{identified_faces}** ({identified_faces/max(total_faces_detected, 1)*100:.1f}%)")
                            st.markdown(f"- Khu√¥n m·∫∑t ch∆∞a nh·∫≠n d·∫°ng: **{unknown_faces}** ({unknown_faces/max(total_faces_detected, 1)*100:.1f}%)")
                            
                        with stats_col2:
                            st.markdown("### Ng∆∞·ªùi xu·∫•t hi·ªán trong video")
                            if people_detected:
                                # S·∫Øp x·∫øp theo s·ªë l·∫ßn xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
                                sorted_people = sorted(people_frame_counts.items(), key=lambda x: x[1], reverse=True)
                                for person, count in sorted_people:
                                    frames_percentage = count / processed_frames * 100
                                    st.markdown(f"- **{person}**: {count} frames ({frames_percentage:.1f}%)")
                            else:
                                st.markdown("Kh√¥ng c√≥ ng∆∞·ªùi n√†o ƒë∆∞·ª£c nh·∫≠n d·∫°ng")
                        
                        # Show final result
                        if result_mode == "Video ho√†n ch·ªânh":
                            # Display processed video
                            st.markdown("### Video k·∫øt qu·∫£")
                            
                            try:
                                # ƒê·∫£m b·∫£o video ƒë∆∞·ª£c ƒë√≥ng tr∆∞·ªõc khi ƒë·ªçc l·∫°i
                                if 'out' in locals():
                                    out.release()
                                    
                                # Ki·ªÉm tra xem file c√≥ t·ªìn t·∫°i v√† c√≥ k√≠ch th∆∞·ªõc kh√¥ng
                                if os.path.exists(output_video_path) and os.path.getsize(output_video_path) > 0:
                                    # Th√™m codec cho ƒë√∫ng ƒë·ªãnh d·∫°ng web
                                    temp_web_path = f"temp_web_{int(time.time())}.mp4"
                                    os.system(f"ffmpeg -y -i {output_video_path} -vcodec libx264 -pix_fmt yuv420p {temp_web_path}")
                                    
                                    if os.path.exists(temp_web_path) and os.path.getsize(temp_web_path) > 0:
                                        # ƒê·ªçc video ƒë√£ x·ª≠ l√Ω v·ªõi ƒë√∫ng ƒë·ªãnh d·∫°ng cho web
                                        video_bytes = open(temp_web_path, 'rb').read()
                                        
                                        # Hi·ªÉn th·ªã video s·ª≠ d·ª•ng component c√≥ s·∫µn c·ªßa Streamlit
                                        st.video(video_bytes)
                                        
                                        # Th√™m n√∫t t·∫£i v·ªÅ v·ªõi style r√µ r√†ng
                                        st.download_button(
                                            label="üì• T·∫£i video k·∫øt qu·∫£",
                                            data=video_bytes,
                                            file_name=f"face_recognition_{int(time.time())}.mp4",
                                            mime="video/mp4",
                                            use_container_width=True
                                        )
                                        
                                        # X√≥a file t·∫°m
                                        try:
                                            os.remove(temp_web_path)
                                        except:
                                            pass
                                    else:
                                        st.error("Kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi video sang ƒë·ªãnh d·∫°ng web. H√£y th·ª≠ l·∫°i.")
                                        st.info("B·∫°n c√≥ th·ªÉ c·∫ßn c√†i ƒë·∫∑t ffmpeg: `pip install ffmpeg-python`")
                                else:
                                    st.error("Video k·∫øt qu·∫£ kh√¥ng t·ªìn t·∫°i ho·∫∑c r·ªóng. Vui l√≤ng th·ª≠ l·∫°i qu√° tr√¨nh x·ª≠ l√Ω.")
                            except Exception as e:
                                st.error(f"L·ªói khi hi·ªÉn th·ªã video: {str(e)}")
                                st.info("Vui l√≤ng th·ª≠ l·∫°i v·ªõi video kh√°c ho·∫∑c ch·ªçn ch·∫ø ƒë·ªô hi·ªÉn th·ªã t·ª´ng frame.")
                                
                            # Clean up temporary files
                            try:
                                os.remove(output_video_path)
                            except:
                                pass
                        else:
                            # Display the highlights
                            st.markdown("### C√°c frame ƒë√°ng ch√∫ √Ω")
                            
                            # Find frames with the most recognized faces
                            highlight_frames = []
                            for result in all_results:
                                frame_idx, faces, names, scores, frame = result
                                recognized_count = sum(1 for name in names if name != "Unknown")
                                if recognized_count > 0:
                                    highlight_frames.append((recognized_count, frame_idx, frame))
                            
                            # Sort by recognition count (most first)
                            highlight_frames.sort(reverse=True)
                            
                            # Display top highlights (max 5)
                            max_highlights = min(5, len(highlight_frames))
                            if max_highlights > 0:
                                highlight_cols = st.columns(max_highlights)
                                for i in range(max_highlights):
                                    if i < len(highlight_frames):
                                        count, frame_idx, frame = highlight_frames[i]
                                        with highlight_cols[i]:
                                            st.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), use_container_width=True)
                                            st.caption(f"Frame {frame_idx} - {count} ng∆∞·ªùi")
                            else:
                                st.warning("Kh√¥ng t√¨m th·∫•y frame n√†o v·ªõi khu√¥n m·∫∑t ƒë∆∞·ª£c nh·∫≠n d·∫°ng")
                            
                    # Clean up temporary file
                    try:
                        os.remove(temp_video_path)
                    except:
                        pass
    elif mode == "üé• Video tr·ª±c ti·∫øp":
        st.markdown("### üé• Video tr·ª±c ti·∫øp t·ª´ camera")
        
        # Performance settings
        col1, col2 = st.columns(2)
        with col1:
            resolution = st.selectbox("ƒê·ªô ph√¢n gi·∫£i:", 
                                    ["640x480", "800x600", "1280x720"],
                                    index=0)
        with col2:
            process_rate = st.slider("T·ªëc ƒë·ªô x·ª≠ l√Ω:", 1, 6, 2, 1)
        
        # Parse resolution
        w, h = map(int, resolution.split('x'))
        
        # Control buttons
        col1, col2 = st.columns([1, 4])
        with col1:
            start_button = st.button("B·∫Øt ƒë·∫ßu", type="primary", use_container_width=True)
        with col2:
            stop_button = st.button("D·ª´ng", use_container_width=True, key="stop_button")
        
        # Initialize session state
        if 'video_running' not in st.session_state:
            st.session_state.video_running = False
        
        # Initialize face tracker (faster settings)
        face_tracker = StableFaceTracker(max_distance=40.0, min_frames=2, max_missing_frames=8)
        
        # Video display area
        video_placeholder = st.empty()
        info_placeholder = st.empty()
        
        # Start video stream
        if start_button:
            st.session_state.video_running = True
        
        if stop_button:
            st.session_state.video_running = False
        
        # Video processing loop
        if st.session_state.video_running:
            # Hi·ªÉn th·ªã loading state
            video_placeholder.markdown("### ‚è≥ ƒêang m·ªü camera...")
            
            # Try different camera indices
            cap = None
            for idx in range(3):  # Try index 0, 1, 2
                cap = cv2.VideoCapture(idx)
                if cap.isOpened():
                    break
                cap.release()
            
            if cap is None or not cap.isOpened():
                st.error("Kh√¥ng th·ªÉ m·ªü camera. Vui l√≤ng ki·ªÉm tra l·∫°i k·∫øt n·ªëi camera.")
            else:
                # Set camera resolution
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, w)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, h)
                cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                
                # Hi·ªÉn th·ªã th√†nh c√¥ng
                video_placeholder.markdown("‚úÖ Camera ƒë√£ s·∫µn s√†ng")
                time.sleep(0.5)  # Hi·ªÉn th·ªã th√¥ng b√°o trong 0.5 gi√¢y
                
                # FPS counter and frame processing counter
                fps_start_time = time.time()
                fps_counter = 0
                current_fps = 0
                frame_num = 0
                
                # Initialize variable to track last processed frame
                result_frame = None
                names = []
                scores = []
                
                while st.session_state.video_running:
                    ret, frame = cap.read()
                    
                    if not ret:
                        st.error("Kh√¥ng th·ªÉ ƒë·ªçc khung h√¨nh t·ª´ camera!")
                        break
                    
                    # Flip frame horizontally for mirror effect
                    frame = cv2.flip(frame, 1)
                    
                    # Process every Nth frame based on speed setting
                    if frame_num % process_rate == 0:
                        # Detect faces
                        faces, aligned_faces = face_detector.detect(frame)
                        
                        # Update face tracker
                        stable_faces = face_tracker.update(faces, frame_num)
                        
                        # Only recognize stable faces
                        stable_names = []
                        stable_scores = []
                        
                        for face in stable_faces:
                            if face['is_stable']:
                                # Find corresponding aligned face (optimized matching)
                                best_face_img = None
                                min_dist = float('inf')
                                
                                for i, detected_face in enumerate(faces):
                                    # Simplified distance calculation
                                    if i < len(aligned_faces):
                                        dist = np.sum(np.abs(detected_face[:4] - face['box']))
                                        if dist < min_dist:
                                            min_dist = dist
                                            best_face_img = aligned_faces[i]
                                
                                if best_face_img is not None:
                                    name, score = face_recognizer.identify(best_face_img)
                                    stable_names.append(name)
                                    stable_scores.append(score)
                        
                        # Draw stable results (optimized function)
                        result_frame = draw_stable_results(frame, stable_faces, stable_names, stable_scores)
                        names = stable_names
                        scores = stable_scores
                    
                    frame_num += 1
                    
                    # Use the last processed frame if available, otherwise use raw frame
                    display_frame = result_frame if result_frame is not None else frame
                    
                    # Calculate FPS
                    fps_counter += 1
                    if time.time() - fps_start_time >= 1.0:
                        current_fps = fps_counter
                        fps_counter = 0
                        fps_start_time = time.time()
                    
                    # Add FPS text to frame (simplified drawing)
                    cv2.putText(display_frame, f"FPS: {current_fps}", (10, 30),
                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                    
                    # Convert to RGB for display only once
                    display_frame_rgb = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
                    
                    # Display frame
                    video_placeholder.image(display_frame_rgb, channels="RGB", use_container_width=True)
                    
                    # Display recognition info (only show if needed)
                    if frame_num % (process_rate * 2) == 0:  # Update info less frequently
                        if names:
                            with info_placeholder:
                                # Count unique faces (only show green boxes count)
                                stable_count = sum(1 for face in stable_faces if face['is_stable'])
                                actual_people = min(stable_count, len(names))
                                
                                cols = st.columns(min(actual_people, 3))
                                for i in range(actual_people):
                                    if i < len(names) and i < len(scores):
                                        with cols[i % 3]:
                                            status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if names[i] != "Unknown" else "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c"
                                            st.markdown(f"""
                                            **Ng∆∞·ªùi {i+1}**  
                                            {names[i]}  
                                            Score: {scores[i]:.2f}  
                                            {status}
                                            """)
                        else:
                            info_placeholder.markdown("*ƒêang ph√°t hi·ªán khu√¥n m·∫∑t...*")
                    
                    # Check if stop button was pressed
                    if not st.session_state.video_running:
                        break
                
                # Release camera
                cap.release()
                video_placeholder.markdown("**Camera ƒë√£ ƒë∆∞·ª£c ƒë√≥ng**")
        
        elif not st.session_state.video_running and not start_button:
            video_placeholder.markdown("""
            ## üé• Ch·∫ø ƒë·ªô Video Tr·ª±c Ti·∫øp
            
            **T√≠nh nƒÉng:**
            - Hi·ªÉn th·ªã khung khu√¥n m·∫∑t ·ªïn ƒë·ªãnh, kh√¥ng ch·ªõp nh√°y
            - T·ª± ƒë·ªông nh·∫≠n d·∫°ng khu√¥n m·∫∑t trong th·ªùi gian th·ª±c
            - Hi·ªÉn th·ªã FPS ƒë·ªÉ theo d√µi hi·ªáu su·∫•t
            
            **T√πy ch·ªçn hi·ªáu su·∫•t:**
            - ƒê·ªô ph√¢n gi·∫£i th·∫•p = nhanh h∆°n
            - T·ªëc ƒë·ªô x·ª≠ l√Ω cao = x·ª≠ l√Ω nhi·ªÅu frame h∆°n/gi√¢y
            
            Nh·∫•n **B·∫Øt ƒë·∫ßu** ƒë·ªÉ m·ªü camera.
            """)