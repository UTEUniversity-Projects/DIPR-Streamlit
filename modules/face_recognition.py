import streamlit as st
import cv2
import numpy as np
import time
import os
from typing import List, Dict, Tuple, Optional

@st.cache_resource
def load_face_detector():
    from utils.face_utils import FaceDetector
    return FaceDetector("models/face_detection_yunet_2023mar.onnx")

@st.cache_resource
def load_face_recognizer():
    from utils.face_utils import FaceRecognizer
    return FaceRecognizer("models/face_recognition_sface_2021dec.onnx", "data/db_embeddings.pkl")

class StableFaceTracker:
    def __init__(self, max_distance: float = 50.0, min_frames: int = 5, max_missing_frames: int = 15):
        self.tracked_faces: List[Dict] = []
        self.max_distance = max_distance
        self.min_frames = min_frames
        self.max_missing_frames = max_missing_frames
        
    def update(self, detected_faces: np.ndarray, frame_num: int) -> List[Dict]:
        """Update tracked faces with new detections"""
        # If no detections, gradually remove tracked faces
        if len(detected_faces) == 0:
            # Remove faces that have been missing for too long
            self.tracked_faces = [face for face in self.tracked_faces 
                                if frame_num - face['last_seen'] < self.max_missing_frames]
            
            # Gradually decrease confidence for missing faces
            for face in self.tracked_faces:
                face['score'] *= 0.95  # Decay score each frame
                face['frame_count'] = max(0, face['frame_count'] - 1)
                if face['frame_count'] < self.min_frames:
                    face['is_stable'] = False
            
            return self.tracked_faces
        
        # Track existing faces first
        if len(self.tracked_faces) > 0:
            # Mark all as unmatched initially
            unmatched_tracked = list(range(len(self.tracked_faces)))
            matched_new = set()
            
            # Find matches between new and tracked faces
            for j, new_face in enumerate(detected_faces):
                best_match_idx = None
                min_dist = float('inf')
                
                for i in unmatched_tracked:
                    tracked = self.tracked_faces[i]
                    dist = self._calculate_distance(new_face[:4], tracked['box'])
                    if dist < min_dist and dist < self.max_distance:
                        min_dist = dist
                        best_match_idx = i
                
                if best_match_idx is not None:
                    # Update existing face
                    tracked = self.tracked_faces[best_match_idx]
                    tracked['box'] = self._smooth_box(tracked['box'], new_face[:4])
                    tracked['score'] = 0.9 * tracked['score'] + 0.1 * new_face[4]
                    tracked['frame_count'] += 1
                    tracked['last_seen'] = frame_num
                    
                    # Mark as stable if seen for enough frames
                    if tracked['frame_count'] >= self.min_frames:
                        tracked['is_stable'] = True
                    
                    unmatched_tracked.remove(best_match_idx)
                    matched_new.add(j)
            
            # Add new faces
            for j, new_face in enumerate(detected_faces):
                if j not in matched_new:
                    self.tracked_faces.append({
                        'box': new_face[:4],
                        'score': new_face[4],
                        'frame_count': 1,
                        'last_seen': frame_num,
                        'is_stable': False
                    })
        else:
            # Add all detected faces as new
            for new_face in detected_faces:
                self.tracked_faces.append({
                    'box': new_face[:4],
                    'score': new_face[4],
                    'frame_count': 1,
                    'last_seen': frame_num,
                    'is_stable': False
                })
        
        # Remove old faces
        self.tracked_faces = [face for face in self.tracked_faces 
                            if frame_num - face['last_seen'] < self.max_missing_frames]
        
        return self.tracked_faces
    
    def _calculate_distance(self, box1: np.ndarray, box2: np.ndarray) -> float:
        """Calculate distance between two face boxes"""
        x1, y1, w1, h1 = box1
        x2, y2, w2, h2 = box2
        center1 = (x1 + w1/2, y1 + h1/2)
        center2 = (x2 + w2/2, y2 + h2/2)
        return np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)
    
    def _smooth_box(self, old_box: np.ndarray, new_box: np.ndarray, alpha: float = 0.3) -> np.ndarray:
        """Smooth transition between box positions"""
        return old_box * (1 - alpha) + new_box * alpha

def draw_stable_results(frame: np.ndarray, stable_faces: List[Dict], 
                       names: List[str] = None, scores: List[float] = None) -> np.ndarray:
    """Draw detection and recognition results on the frame with stability (OPTIMIZED)"""
    result = frame.copy()
    
    # Pre-calculate font settings
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.7
    thickness = 2
    
    # Draw only stable faces
    stable_index = 0
    for face in stable_faces:
        if not face['is_stable']:
            continue
            
        x, y, w, h = map(int, face['box'])
        
        # Get name and score if available
        name = names[stable_index] if names and stable_index < len(names) else None
        score = scores[stable_index] if scores and stable_index < len(scores) else None
        
        # Choose color based on recognition status
        color = (0, 255, 0)  # Green for both detected and recognized faces
        text_color = (255, 255, 255)  # White text
        
        # Draw face rectangle (simpler draw method)
        cv2.rectangle(result, (x, y), (x+w, y+h), color, 2)
        
        # Draw label if available
        if name is not None:
            label = f"{name} ({score:.2f})" if score is not None else name
            
            # Get text size once
            (text_width, text_height), baseline = cv2.getTextSize(label, font, font_scale, thickness)
            
            # Calculate text position
            text_y = y - 10 if y - 10 > 10 else y + h + 20
            
            # Draw text background (simpler rectangle)
            cv2.rectangle(result, 
                         (x, text_y - text_height - baseline), 
                         (x + text_width + 5, text_y + baseline),
                         color, cv2.FILLED)
            
            # Draw text
            cv2.putText(result, label, (x + 2, text_y - 5),
                        font, font_scale, text_color, thickness)
        
        stable_index += 1
    
    return result

def show():
    # Th√™m ph·∫ßn gi·ªõi thi·ªáu v√† h∆∞·ªõng d·∫´n
    with st.expander("üîç Gi·ªõi thi·ªáu v·ªÅ nh·∫≠n d·∫°ng khu√¥n m·∫∑t", expanded=False):
        st.markdown("""
        ### Gi·ªõi thi·ªáu v·ªÅ nh·∫≠n d·∫°ng khu√¥n m·∫∑t
        
        T√≠nh nƒÉng nh·∫≠n d·∫°ng khu√¥n m·∫∑t trong ·ª©ng d·ª•ng n√†y s·ª≠ d·ª•ng ki·∫øn tr√∫c hai giai ƒëo·∫°n hi·ªán ƒë·∫°i:
        
        1. **YuNet Face Detector**: M√¥ h√¨nh ph√°t hi·ªán khu√¥n m·∫∑t d·ª±a tr√™n CNN (Convolutional Neural Network) ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a
           - Ki·∫øn tr√∫c: Lightweight network v·ªõi c√°c kh·ªëi ResNet c·∫£i ti·∫øn
           - Hi·ªáu su·∫•t: ƒê·ªô ch√≠nh x√°c cao v√† x·ª≠ l√Ω th·ªùi gian th·ª±c tr√™n CPU
           - ƒê·ªãnh d·∫°ng: ONNX v·ªõi k√≠ch th∆∞·ªõc nh·ªè (< 1MB)
        
        2. **SFace Recognition**: M√¥ h√¨nh nh·∫≠n d·∫°ng khu√¥n m·∫∑t v·ªõi ƒë·ªô ch√≠nh x√°c cao
           - Ki·∫øn tr√∫c: D·ª±a tr√™n m·∫°ng ResNet-50 c·∫£i ti·∫øn cho c√°c embedding vector 128 chi·ªÅu
           - Training: ƒê∆∞·ª£c hu·∫•n luy·ªán tr√™n dataset l·ªõn v·ªõi loss function ƒë·∫∑c bi·ªát gi√∫p ph√¢n bi·ªát t·ªët c√°c khu√¥n m·∫∑t
           - Ph∆∞∆°ng ph√°p: So s√°nh vector embedding v·ªõi c√°c vector trong c∆° s·ªü d·ªØ li·ªáu
        
        ·ª®ng d·ª•ng c≈©ng t√≠ch h·ª£p thu·∫≠t to√°n theo d√µi khu√¥n m·∫∑t (face tracking) ƒë·ªÉ ƒë·∫£m b·∫£o ·ªïn ƒë·ªãnh khi nh·∫≠n d·∫°ng video th·ªùi gian th·ª±c, gi·∫£m hi·ªán t∆∞·ª£ng "ch·ªõp nh√°y" khi x√°c ƒë·ªãnh danh t√≠nh.
        
        **C√°c t√≠nh nƒÉng ch√≠nh:**
        - Nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ ·∫£nh tƒ©nh (t·∫£i l√™n ho·∫∑c ch·ª•p t·ª´ webcam)
        - Nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ video t·∫£i l√™n (nhi·ªÅu ƒë·ªãnh d·∫°ng nh∆∞ MP4, AVI, MOV, v.v.)
        - Ph√°t hi·ªán v√† nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ webcam theo th·ªùi gian th·ª±c
        - Theo d√µi khu√¥n m·∫∑t ·ªïn ƒë·ªãnh gi√∫p gi·∫£m ƒë√°ng k·ªÉ sai s·ªë trong nh·∫≠n d·∫°ng
        
        **·ª®ng d·ª•ng th·ª±c t·∫ø:**
        - H·ªá th·ªëng b·∫£o m·∫≠t v√† ki·ªÉm so√°t truy c·∫≠p
        - H·ªá th·ªëng ƒëi·ªÉm danh t·ª± ƒë·ªông
        - Ph√¢n t√≠ch video gi√°m s√°t
        - Nh·∫≠n d·∫°ng khu√¥n m·∫∑t trong d·ªØ li·ªáu h√¨nh ·∫£nh v√† video l·ªõn
        - Tr·∫£i nghi·ªám c√° nh√¢n h√≥a trong c√°c h·ªá th·ªëng th√¥ng minh
        - X√°c th·ª±c danh t√≠nh kh√¥ng ti·∫øp x√∫c
        """)
            
    with st.expander("üìã H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng", expanded=False):
        st.markdown("""
        ### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng
        
        #### 1. Ch·∫ø ƒë·ªô ·∫£nh tƒ©nh
        - **Upload ·∫£nh**: T·∫£i l√™n ·∫£nh ch·ª©a khu√¥n m·∫∑t c·∫ßn nh·∫≠n d·∫°ng
        - **Ch·ª•p t·ª´ webcam**: Ch·ª•p ·∫£nh tr·ª±c ti·∫øp t·ª´ webcam ƒë·ªÉ nh·∫≠n d·∫°ng
        
        #### 2. Ch·∫ø ƒë·ªô video t·∫£i l√™n
        - **Upload video**: T·∫£i l√™n video c√≥ ƒë·ªãnh d·∫°ng MP4, AVI, MOV, MKV, v.v.
        - **ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô x·ª≠ l√Ω**: Ch·ªçn t·ªëc ƒë·ªô x·ª≠ l√Ω (Ch·∫ø ƒë·ªô nhanh / Ch·∫ø ƒë·ªô ch·∫•t l∆∞·ª£ng cao)
        - **T√πy ch·ªçn hi·ªÉn th·ªã**: Hi·ªÉn th·ªã k·∫øt qu·∫£ theo t·ª´ng frame ho·∫∑c video ho√†n ch·ªânh
        - **Thanh ƒëi·ªÅu khi·ªÉn video**: T·∫°m d·ª´ng, tua ƒëi, tua l·∫°i, v.v.
        
        #### 3. Ch·∫ø ƒë·ªô video tr·ª±c ti·∫øp
        - **B·∫Øt ƒë·∫ßu**: M·ªü camera v√† b·∫Øt ƒë·∫ßu ph√°t hi·ªán khu√¥n m·∫∑t
        - **D·ª´ng**: D·ª´ng qu√° tr√¨nh nh·∫≠n d·∫°ng v√† ƒë√≥ng camera
        - **ƒêi·ªÅu ch·ªânh ƒë·ªô ph√¢n gi·∫£i**: Ch·ªçn ƒë·ªô ph√¢n gi·∫£i camera ph√π h·ª£p
        - **T·ªëc ƒë·ªô x·ª≠ l√Ω**: ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô x·ª≠ l√Ω khung h√¨nh (gi√° tr·ªã th·∫•p h∆°n = x·ª≠ l√Ω nhi·ªÅu frame h∆°n)
        
        #### M·∫πo s·ª≠ d·ª•ng:
        - **√Ånh s√°ng**: ƒê·∫£m b·∫£o khu√¥n m·∫∑t ƒë∆∞·ª£c chi·∫øu s√°ng t·ªët
        - **G√≥c nh√¨n**: N√™n ch·ªçn g√≥c th·∫≥ng ho·∫∑c nghi√™ng nh·∫π ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët nh·∫•t
        - **Kho·∫£ng c√°ch**: Khu√¥n m·∫∑t n√™n chi·∫øm kho·∫£ng 10-15% khung h√¨nh ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªëi ∆∞u
        - **Ch·∫•t l∆∞·ª£ng video**: ∆Øu ti√™n video c√≥ ƒë·ªô ph√¢n gi·∫£i cao (720p tr·ªü l√™n) v√† √≠t nhi·ªÖu
        - **ƒêƒÉng k√Ω m·∫∑t m·ªõi**: N·∫øu khu√¥n m·∫∑t ch∆∞a ƒë∆∞·ª£c nh·∫≠n d·∫°ng, s·ª≠ d·ª•ng ch·ª©c nƒÉng "ƒêƒÉng k√Ω khu√¥n m·∫∑t m·ªõi"
        
        #### X·ª≠ l√Ω l·ªói:
        - **Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c**: ƒê·∫£m b·∫£o khu√¥n m·∫∑t ƒë√£ ƒë∆∞·ª£c ƒëƒÉng k√Ω trong h·ªá th·ªëng
        - **Nh·∫≠n d·∫°ng sai**: C·∫≠p nh·∫≠t database v·ªõi nhi·ªÅu m·∫´u khu√¥n m·∫∑t h∆°n
        - **L·ªói x·ª≠ l√Ω video**: Th·ª≠ chuy·ªÉn ƒë·ªïi video sang MP4 ho·∫∑c gi·∫£m ƒë·ªô ph√¢n gi·∫£i
        - **Kh√¥ng hi·ªÉn th·ªã camera**: Ki·ªÉm tra quy·ªÅn truy c·∫≠p camera trong tr√¨nh duy·ªát
        """)
    
    # Load models
    face_detector = load_face_detector()
    face_recognizer = load_face_recognizer()
    
    # Choose input mode
    mode = st.radio("Ch·ªçn ch·∫ø ƒë·ªô nh·∫≠n d·∫°ng:", ["üì∏ ·∫¢nh tƒ©nh", "üé¨ Video t·∫£i l√™n", "üé• Video tr·ª±c ti·∫øp"])
    
    # Process based on selected mode
    if mode == "üì∏ ·∫¢nh tƒ©nh":
        # Image input methods
        input_method = st.radio("Ch·ªçn ngu·ªìn ·∫£nh:", ["Upload ·∫£nh", "Ch·ª•p t·ª´ webcam"])
        
        if input_method == "Ch·ª•p t·ª´ webcam":
            # Camera capture interface  
            picture = st.camera_input("Ch·ª•p ·∫£nh khu√¥n m·∫∑t")
            
            if picture:
                # Convert to OpenCV format
                bytes_data = picture.getvalue()
                img = cv2.imdecode(np.frombuffer(bytes_data, np.uint8), cv2.IMREAD_COLOR)
                
                # Create two columns for original and result images
                col1, col2 = st.columns(2)
                
                # Display original image
                with col1:
                    st.subheader("·∫¢nh g·ªëc")
                    st.image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), use_container_width=True)
                
                # Process image
                with st.spinner("ƒêang nh·∫≠n d·∫°ng khu√¥n m·∫∑t..."):
                    faces, aligned_faces = face_detector.detect(img)
                    
                    if len(faces) > 0:
                        names = []
                        scores = []
                        
                        # Recognize faces (limit to 5)
                        for face_img in aligned_faces[:5]:
                            name, score = face_recognizer.identify(face_img)
                            names.append(name)
                            scores.append(score)
                        
                        # Draw results
                        from utils.face_utils import draw_results
                        result_img = draw_results(img, faces[:5], names, scores)
                    else:
                        result_img = img  # No faces detected
                        names = []
                        scores = []
                    
                    # Display result
                    with col2:
                        st.subheader("K·∫øt qu·∫£")
                        st.image(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB), use_container_width=True)
                    
                    # Display information below images
                    if names:
                        st.markdown("### Th√¥ng tin nh·∫≠n d·∫°ng:")
                        for i, (name, score) in enumerate(zip(names, scores)):
                            status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if name != "Unknown" else "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c"
                            st.write(f"**Ng∆∞·ªùi {i+1}:** {name} ({score:.2f}) - {status}")
                    else:
                        st.warning("Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t n√†o trong ·∫£nh!")
        
        else:  # Upload image mode
            uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh ch·ª©a khu√¥n m·∫∑t", type=["jpg", "jpeg", "png"])
            
            if uploaded_file is not None:
                # Read image
                file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
                img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
                
                # Create two columns for original and result images
                col1, col2 = st.columns(2)
                
                # Display original image
                with col1:
                    st.subheader("·∫¢nh g·ªëc")
                    st.image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), use_container_width=True)
                
                # Process image
                with st.spinner("ƒêang nh·∫≠n d·∫°ng khu√¥n m·∫∑t..."):
                    faces, aligned_faces = face_detector.detect(img)
                    
                    if len(faces) > 0:
                        names = []
                        scores = []
                        
                        # Recognize faces (limit to 5)
                        for face_img in aligned_faces[:5]:
                            name, score = face_recognizer.identify(face_img)
                            names.append(name)
                            scores.append(score)
                        
                        # Draw results
                        from utils.face_utils import draw_results
                        result_img = draw_results(img, faces[:5], names, scores)
                    else:
                        result_img = img  # No faces detected
                        names = []
                        scores = []
                    
                    # Display result
                    with col2:
                        st.subheader("K·∫øt qu·∫£")
                        st.image(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB), use_container_width=True)
                    
                    # Display information below images
                    if names:
                        st.markdown("### Th√¥ng tin nh·∫≠n d·∫°ng:")
                        for i, (name, score) in enumerate(zip(names, scores)):
                            status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if name != "Unknown" else "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c"
                            st.write(f"**Ng∆∞·ªùi {i+1}:** {name} ({score:.2f}) - {status}")
                    else:
                        st.warning("Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t n√†o trong ·∫£nh!")
        
    elif mode == "üé¨ Video t·∫£i l√™n":
        # Video upload section with improved explanation
        st.subheader("Nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ video")
        
        # Add helpful explanation
        st.markdown("""
        T√≠nh nƒÉng n√†y cho ph√©p nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ file video t·∫£i l√™n.
        Qu√° tr√¨nh x·ª≠ l√Ω s·∫Ω di·ªÖn ra theo th·ªùi gian th·ª±c - b·∫°n s·∫Ω th·∫•y k·∫øt qu·∫£ ngay khi video ƒëang ch·∫°y!
        """)
        
        # Upload video file with clearer instructions
        video_file = st.file_uploader(
            "T·∫£i l√™n video ch·ª©a khu√¥n m·∫∑t c·∫ßn nh·∫≠n d·∫°ng", 
            type=["mp4", "mov", "avi", "mkv", "wmv"],
            help="H·ªó tr·ª£ c√°c ƒë·ªãnh d·∫°ng: MP4, MOV, AVI, MKV, WMV. N√™n s·ª≠ d·ª•ng video MP4 ƒë·ªÉ c√≥ hi·ªáu su·∫•t t·ªët nh·∫•t."
        )
        
        if video_file is not None:
            # Process video in realtime mode
            process_video_realtime(video_file, face_detector, face_recognizer)
            
    elif mode == "üé• Video tr·ª±c ti·∫øp":
        st.markdown("### üé• Video tr·ª±c ti·∫øp t·ª´ camera")
        
        # Performance settings
        col1, col2 = st.columns(2)
        with col1:
            resolution = st.selectbox("ƒê·ªô ph√¢n gi·∫£i:", 
                                    ["640x480", "800x600", "1280x720"],
                                    index=0)
        with col2:
            process_rate = st.slider("T·ªëc ƒë·ªô x·ª≠ l√Ω:", 1, 6, 2, 1)
        
        # Parse resolution
        w, h = map(int, resolution.split('x'))
        
        # Control buttons
        col1, col2 = st.columns([1, 4])
        with col1:
            start_button = st.button("B·∫Øt ƒë·∫ßu", type="primary", use_container_width=True)
        with col2:
            stop_button = st.button("D·ª´ng", use_container_width=True, key="stop_button")
        
        # Initialize session state
        if 'video_running' not in st.session_state:
            st.session_state.video_running = False
        
        # Initialize face tracker (faster settings)
        face_tracker = StableFaceTracker(max_distance=40.0, min_frames=2, max_missing_frames=8)
        
        # Video display area
        video_placeholder = st.empty()
        info_placeholder = st.empty()
        
        # Start video stream
        if start_button:
            st.session_state.video_running = True
        
        if stop_button:
            st.session_state.video_running = False
        
        # Video processing loop
        if st.session_state.video_running:
            # Hi·ªÉn th·ªã loading state
            video_placeholder.markdown("### ‚è≥ ƒêang m·ªü camera...")
            
            # Try different camera indices
            cap = None
            for idx in range(3):  # Try index 0, 1, 2
                cap = cv2.VideoCapture(idx)
                if cap.isOpened():
                    break
                cap.release()
            
            if cap is None or not cap.isOpened():
                st.error("Kh√¥ng th·ªÉ m·ªü camera. Vui l√≤ng ki·ªÉm tra l·∫°i k·∫øt n·ªëi camera.")
            else:
                # Set camera resolution
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, w)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, h)
                cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                
                # Hi·ªÉn th·ªã th√†nh c√¥ng
                video_placeholder.markdown("‚úÖ Camera ƒë√£ s·∫µn s√†ng")
                time.sleep(0.5)  # Hi·ªÉn th·ªã th√¥ng b√°o trong 0.5 gi√¢y
                
                # FPS counter and frame processing counter
                fps_start_time = time.time()
                fps_counter = 0
                current_fps = 0
                frame_num = 0
                
                # Initialize variable to track last processed frame
                result_frame = None
                names = []
                scores = []
                
                while st.session_state.video_running:
                    ret, frame = cap.read()
                    
                    if not ret:
                        st.error("Kh√¥ng th·ªÉ ƒë·ªçc khung h√¨nh t·ª´ camera!")
                        break
                    
                    # Flip frame horizontally for mirror effect
                    frame = cv2.flip(frame, 1)
                    
                    # Process every Nth frame based on speed setting
                    if frame_num % process_rate == 0:
                        # Detect faces
                        faces, aligned_faces = face_detector.detect(frame)
                        
                        # Update face tracker
                        stable_faces = face_tracker.update(faces, frame_num)
                        
                        # Only recognize stable faces
                        stable_names = []
                        stable_scores = []
                        
                        for face in stable_faces:
                            if face['is_stable']:
                                # Find corresponding aligned face (optimized matching)
                                best_face_img = None
                                min_dist = float('inf')
                                
                                for i, detected_face in enumerate(faces):
                                    # Simplified distance calculation
                                    if i < len(aligned_faces):
                                        dist = np.sum(np.abs(detected_face[:4] - face['box']))
                                        if dist < min_dist:
                                            min_dist = dist
                                            best_face_img = aligned_faces[i]
                                
                                if best_face_img is not None:
                                    name, score = face_recognizer.identify(best_face_img)
                                    stable_names.append(name)
                                    stable_scores.append(score)
                        
                        # Draw stable results (optimized function)
                        result_frame = draw_stable_results(frame, stable_faces, stable_names, stable_scores)
                        names = stable_names
                        scores = stable_scores
                    
                    frame_num += 1
                    
                    # Use the last processed frame if available, otherwise use raw frame
                    display_frame = result_frame if result_frame is not None else frame
                    
                    # Calculate FPS
                    fps_counter += 1
                    if time.time() - fps_start_time >= 1.0:
                        current_fps = fps_counter
                        fps_counter = 0
                        fps_start_time = time.time()
                    
                    # Add FPS text to frame (simplified drawing)
                    cv2.putText(display_frame, f"FPS: {current_fps}", (10, 30),
                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                    
                    # Convert to RGB for display only once
                    display_frame_rgb = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
                    
                    # Display frame
                    video_placeholder.image(display_frame_rgb, channels="RGB", use_container_width=True)
                    
                    # Display recognition info (only show if needed)
                    if frame_num % (process_rate * 2) == 0:  # Update info less frequently
                        if names:
                            with info_placeholder:
                                # Count unique faces (only show green boxes count)
                                stable_count = sum(1 for face in stable_faces if face['is_stable'])
                                actual_people = min(stable_count, len(names))
                                
                                cols = st.columns(min(actual_people, 3))
                                for i in range(actual_people):
                                    if i < len(names) and i < len(scores):
                                        with cols[i % 3]:
                                            status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if names[i] != "Unknown" else "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c"
                                            st.markdown(f"""
                                            **Ng∆∞·ªùi {i+1}**  
                                            {names[i]}  
                                            Score: {scores[i]:.2f}  
                                            {status}
                                            """)
                        else:
                            info_placeholder.markdown("*ƒêang ph√°t hi·ªán khu√¥n m·∫∑t...*")
                    
                    # Check if stop button was pressed
                    if not st.session_state.video_running:
                        break
                
                # Release camera
                cap.release()
                video_placeholder.markdown("**Camera ƒë√£ ƒë∆∞·ª£c ƒë√≥ng**")
        
        elif not st.session_state.video_running and not start_button:
            video_placeholder.markdown("""
            ## üé• Ch·∫ø ƒë·ªô Video Tr·ª±c Ti·∫øp
            
            **T√≠nh nƒÉng:**
            - Hi·ªÉn th·ªã khung khu√¥n m·∫∑t ·ªïn ƒë·ªãnh, kh√¥ng ch·ªõp nh√°y
            - T·ª± ƒë·ªông nh·∫≠n d·∫°ng khu√¥n m·∫∑t trong th·ªùi gian th·ª±c
            - Hi·ªÉn th·ªã FPS ƒë·ªÉ theo d√µi hi·ªáu su·∫•t
            
            **T√πy ch·ªçn hi·ªáu su·∫•t:**
            - ƒê·ªô ph√¢n gi·∫£i th·∫•p = nhanh h∆°n
            - T·ªëc ƒë·ªô x·ª≠ l√Ω cao = x·ª≠ l√Ω nhi·ªÅu frame h∆°n/gi√¢y
            
            Nh·∫•n **B·∫Øt ƒë·∫ßu** ƒë·ªÉ m·ªü camera.
            """)
            
def process_video_realtime(video_file, face_detector, face_recognizer):
    """
    X·ª≠ l√Ω video theo th·ªùi gian th·ª±c - kh√¥ng c·∫ßn ƒë·ª£i x·ª≠ l√Ω t·∫•t c·∫£ c√°c frame
    """
    import tempfile
    import os
    import threading
    import queue
    import time
    from datetime import timedelta
    
    # T·∫°o file t·∫°m
    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmpfile:
        temp_video_path = tmpfile.name
        # L∆∞u video v√†o file t·∫°m
        tmpfile.write(video_file.getbuffer())
    
    # M·ªü video
    cap = cv2.VideoCapture(temp_video_path)
    if not cap.isOpened():
        st.error("Kh√¥ng th·ªÉ ƒë·ªçc video. Vui l√≤ng th·ª≠ v·ªõi ƒë·ªãnh d·∫°ng kh√°c.")
        try:
            os.remove(temp_video_path)
        except:
            pass
        return
    
    # L·∫•y th√¥ng tin video
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    duration = frame_count / fps if fps > 0 else 0
    
    # Hi·ªÉn th·ªã th√¥ng tin video
    st.info(f"Th√¥ng tin video: {frame_width}x{frame_height}, {fps:.1f} FPS, {frame_count} frames, th·ªùi l∆∞·ª£ng: {timedelta(seconds=duration)}")
    
    # ƒêi·ªÅu ch·ªânh ƒë·ªô nh·∫°y v√† t·ªëc ƒë·ªô x·ª≠ l√Ω
    col1, col2 = st.columns(2)
    with col1:
        confidence_threshold = st.slider(
            "Ng∆∞·ª°ng nh·∫≠n di·ªán:", 
            min_value=0.5, 
            max_value=0.95, 
            value=0.7, 
            step=0.05,
            help="ƒêi·ªÅu ch·ªânh ƒë·ªô nh·∫°y khi nh·∫≠n di·ªán (cao h∆°n = √≠t nh·∫≠n di·ªán sai h∆°n)"
        )
    with col2:
        skip_frames = st.slider(
            "T·ªëc ƒë·ªô x·ª≠ l√Ω:", 
            min_value=1, 
            max_value=10, 
            value=2, 
            step=1,
            help="S·ªë frame b·ªè qua khi x·ª≠ l√Ω (cao h∆°n = nhanh h∆°n nh∆∞ng m·∫•t m·ªôt s·ªë chi ti·∫øt)"
        )
    
    # Kh·ªüi t·∫°o face tracker
    face_tracker = StableFaceTracker(
        max_distance=40.0,
        min_frames=2,
        max_missing_frames=10
    )
    
    # Th√¥ng tin theo d√µi
    stats = {
        'total_faces': 0,
        'identified_faces': 0,
        'unknown_faces': 0,
        'people_detected': set(),
        'people_frames': {}
    }
    
    # Queue ƒë·ªÉ trao ƒë·ªïi d·ªØ li·ªáu gi·ªØa c√°c lu·ªìng
    frame_queue = queue.Queue(maxsize=30)  # Buffer 30 frames
    result_queue = queue.Queue(maxsize=30)
    stop_event = threading.Event()
    
    # Placeholder hi·ªÉn th·ªã
    video_placeholder = st.empty()
    progress_bar = st.progress(0.0)
    info_placeholder = st.empty()
    stats_placeholder = st.empty()
    
    # Thread x·ª≠ l√Ω frame
    def process_frames():
        frame_idx = 0
        processed_idx = 0
        
        while not stop_event.is_set():
            try:
                if frame_queue.empty():
                    time.sleep(0.01)
                    continue
                
                frame, current_pos = frame_queue.get()
                frame_idx += 1
                
                # Ch·ªâ x·ª≠ l√Ω m·ªói N frame theo skip_frames
                if frame_idx % skip_frames == 0:
                    # Face detection
                    start_time = time.time()
                    faces, aligned_faces = face_detector.detect(frame)
                    
                    # Update face tracking
                    stable_faces = face_tracker.update(faces, frame_idx)
                    
                    # Nh·∫≠n di·ªán c√°c khu√¥n m·∫∑t ·ªïn ƒë·ªãnh
                    stable_names = []
                    stable_scores = []
                    
                    if len(faces) > 0:
                        stats['total_faces'] += len(faces)
                        
                        for face in stable_faces:
                            if face['is_stable']:
                                # T√¨m aligned face t∆∞∆°ng ·ª©ng
                                best_face_img = None
                                min_dist = float('inf')
                                
                                for i, detected_face in enumerate(faces):
                                    if i < len(aligned_faces):
                                        dist = np.sum(np.abs(detected_face[:4] - face['box']))
                                        if dist < min_dist:
                                            min_dist = dist
                                            best_face_img = aligned_faces[i]
                                
                                if best_face_img is not None:
                                    name, score = face_recognizer.identify(best_face_img)
                                    
                                    # √Åp d·ª•ng ng∆∞·ª°ng tin c·∫≠y
                                    if score < confidence_threshold:
                                        name = "Unknown"
                                        stats['unknown_faces'] += 1
                                    else:
                                        stats['identified_faces'] += 1
                                        stats['people_detected'].add(name)
                                        # ƒê·∫øm s·ªë frame m·ªói ng∆∞·ªùi xu·∫•t hi·ªán
                                        stats['people_frames'][name] = stats['people_frames'].get(name, 0) + 1
                                    
                                    stable_names.append(name)
                                    stable_scores.append(score)
                    
                    # V·∫Ω k·∫øt qu·∫£
                    result_frame = draw_stable_results(frame, stable_faces, stable_names, stable_scores)
                    
                    # T√≠nh th·ªùi gian x·ª≠ l√Ω
                    process_time = time.time() - start_time
                    fps_text = f"Processing: {1/process_time:.1f} FPS"
                    
                    # Th√™m th√¥ng tin v√†o frame
                    cv2.putText(
                        result_frame, 
                        fps_text, 
                        (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2
                    )
                    
                    # Th√™m v·ªã tr√≠ frame
                    position_text = f"Frame: {frame_idx}/{frame_count} ({current_pos*100:.0f}%)"
                    cv2.putText(
                        result_frame, 
                        position_text, 
                        (10, frame_height - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2
                    )
                    
                    # G·ª≠i k·∫øt qu·∫£ v√†o queue
                    result_queue.put((result_frame, stable_names, stable_scores, current_pos))
                    processed_idx += 1
                
                frame_queue.task_done()
                
            except Exception as e:
                print(f"L·ªói x·ª≠ l√Ω frame: {e}")
                if frame_queue.qsize() > 0:
                    frame_queue.task_done()
                time.sleep(0.1)
    
    # Thread ƒë·ªçc frame
    def read_frames():
        frame_idx = 0
        try:
            while cap.isOpened() and not stop_event.is_set():
                ret, frame = cap.read()
                if not ret:
                    break
                
                # T√≠nh v·ªã tr√≠ t∆∞∆°ng ƒë·ªëi
                current_pos = cap.get(cv2.CAP_PROP_POS_FRAMES) / frame_count
                
                # ƒê∆∞a frame v√†o queue ƒë·ªÉ x·ª≠ l√Ω, v·ªõi ki·ªÉm tra ƒë·ªÉ tr√°nh t·∫Øc ngh·∫Ωn
                if not frame_queue.full():
                    frame_queue.put((frame, current_pos))
                else:
                    # N·∫øu queue ƒë·∫ßy, ƒë·ª£i m·ªôt ch√∫t
                    time.sleep(0.01)
                
                frame_idx += 1
                
        except Exception as e:
            print(f"L·ªói ƒë·ªçc frame: {e}")
        finally:
            # ƒê√°nh d·∫•u ƒë√£ ƒë·ªçc xong
            stop_event.set()
            cap.release()
    
    # N√∫t ƒëi·ªÅu khi·ªÉn
    st.markdown("### ƒêi·ªÅu khi·ªÉn x·ª≠ l√Ω video")
    col1, col2 = st.columns(2)
    
    with col1:
        start_button = st.button("‚ñ∂Ô∏è B·∫Øt ƒë·∫ßu x·ª≠ l√Ω", type="primary", use_container_width=True)
    with col2:
        stop_button = st.button("‚èπÔ∏è D·ª´ng x·ª≠ l√Ω", use_container_width=True)
    
    if start_button:
        # Kh·ªüi ƒë·ªông c√°c thread
        process_thread = threading.Thread(target=process_frames)
        read_thread = threading.Thread(target=read_frames)
        
        process_thread.daemon = True
        read_thread.daemon = True
        
        # B·∫Øt ƒë·∫ßu x·ª≠ l√Ω
        process_thread.start()
        read_thread.start()
        
        start_time = time.time()
        frame_count_displayed = 0
        
        # Hi·ªÉn th·ªã k·∫øt qu·∫£ theo th·ªùi gian th·ª±c
        st.markdown("### Video ƒëang x·ª≠ l√Ω")
        
        # V√≤ng l·∫∑p hi·ªÉn th·ªã k·∫øt qu·∫£
        try:
            while not stop_event.is_set() or not result_queue.empty():
                if stop_button:
                    stop_event.set()
                    st.warning("ƒêang d·ª´ng x·ª≠ l√Ω...")
                    break
                
                if not result_queue.empty():
                    result_frame, names, scores, pos = result_queue.get()
                    frame_count_displayed += 1
                    
                    # C·∫≠p nh·∫≠t video v√† thanh ti·∫øn tr√¨nh
                    progress_bar.progress(pos)
                    video_placeholder.image(
                        cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB),
                        use_container_width=True
                    )
                    
                    # T√≠nh FPS hi·ªÉn th·ªã
                    elapsed = time.time() - start_time
                    display_fps = frame_count_displayed / elapsed if elapsed > 0 else 0
                    
                    # Hi·ªÉn th·ªã th√¥ng tin nh·∫≠n di·ªán hi·ªán t·∫°i
                    if names:
                        info_text = f"**T·ªëc ƒë·ªô hi·ªÉn th·ªã: {display_fps:.1f} FPS | Khu√¥n m·∫∑t nh·∫≠n di·ªán ƒë∆∞·ª£c:**\n\n"
                        for name, score in zip(names, scores):
                            status = "‚úÖ" if name != "Unknown" else "‚ùå"
                            info_text += f"{status} {name} ({score:.2f}) "
                        info_placeholder.markdown(info_text)
                    
                    # Hi·ªÉn th·ªã th·ªëng k√™ t·ªïng h·ª£p
                    if frame_count_displayed % 10 == 0:  # C·∫≠p nh·∫≠t m·ªói 10 frame
                        stats_text = f"""
                        ### Th·ªëng k√™ x·ª≠ l√Ω:
                        - ƒê√£ x·ª≠ l√Ω: {frame_count_displayed} frames ({display_fps:.1f} FPS)
                        - Khu√¥n m·∫∑t ph√°t hi·ªán: {stats['total_faces']}
                        - Khu√¥n m·∫∑t nh·∫≠n di·ªán: {stats['identified_faces']}
                        - Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c: {stats['unknown_faces']}
                        - S·ªë ng∆∞·ªùi nh·∫≠n di·ªán ƒë∆∞·ª£c: {len(stats['people_detected'])}
                        """
                        
                        # Hi·ªÉn th·ªã top 3 ng∆∞·ªùi xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
                        if stats['people_frames']:
                            stats_text += "\n\n**Top ng∆∞·ªùi xu·∫•t hi·ªán nhi·ªÅu nh·∫•t:**\n"
                            sorted_people = sorted(stats['people_frames'].items(), key=lambda x: x[1], reverse=True)
                            for person, count in sorted_people[:3]:
                                stats_text += f"- {person}: {count} frames\n"
                                
                        stats_placeholder.markdown(stats_text)
                    
                    result_queue.task_done()
                else:
                    # N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£ m·ªõi, ƒë·ª£i m·ªôt ch√∫t
                    time.sleep(0.01)
        
        except Exception as e:
            st.error(f"L·ªói hi·ªÉn th·ªã k·∫øt qu·∫£: {e}")
        finally:
            # ƒê·∫£m b·∫£o d·ª´ng t·∫•t c·∫£ lu·ªìng
            stop_event.set()
            
            # ƒê·ª£i c√°c thread k·∫øt th√∫c
            if 'process_thread' in locals() and process_thread.is_alive():
                process_thread.join(timeout=1.0)
            if 'read_thread' in locals() and read_thread.is_alive():
                read_thread.join(timeout=1.0)
            
            # T√≠nh th·ªùi gian ch·∫°y
            run_time = time.time() - start_time
            
            # Th√¥ng b√°o k·∫øt th√∫c
            st.success(f"ƒê√£ ho√†n th√†nh x·ª≠ l√Ω {frame_count_displayed} frames trong {run_time:.1f} gi√¢y!")
            
            # Hi·ªÉn th·ªã th·ªëng k√™ cu·ªëi c√πng
            st.subheader("K·∫øt qu·∫£ nh·∫≠n di·ªán")
            
            # Hi·ªÉn th·ªã c√°c ng∆∞·ªùi ƒë∆∞·ª£c nh·∫≠n di·ªán 
            if stats['people_detected']:
                st.markdown("### Ng∆∞·ªùi xu·∫•t hi·ªán trong video:")
                
                # S·∫Øp x·∫øp theo s·ªë l·∫ßn xu·∫•t hi·ªán
                if stats['people_frames']:
                    people_cols = st.columns(min(3, len(stats['people_frames'])))
                    sorted_people = sorted(stats['people_frames'].items(), key=lambda x: x[1], reverse=True)
                    
                    for i, (person, frames) in enumerate(sorted_people):
                        with people_cols[i % 3]:
                            frame_percent = frames / frame_count_displayed * 100
                            st.metric(
                                label=person,
                                value=f"{frames} frames",
                                delta=f"{frame_percent:.1f}%"
                            )
            else:
                st.warning("Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c ng∆∞·ªùi n√†o trong video.")
    
    # D·ªçn d·∫πp
    try:
        os.remove(temp_video_path)
    except:
        pass