import streamlit as st
import cv2
import numpy as np
import time
import os
from typing import List, Dict, Tuple, Optional

@st.cache_resource
def load_face_detector():
    from utils.face_utils import FaceDetector
    return FaceDetector("models/face_detection_yunet_2023mar.onnx")

@st.cache_resource(ttl=10)  # Time to live: ch·ªâ 10 gi√¢y ƒë·ªÉ t·∫£i l·∫°i nhanh h∆°n
def load_face_recognizer():
    db_path = "data/db_embeddings.pkl"
    # Th√™m mtime l√†m tham s·ªë ƒë·ªÉ Streamlit bi·∫øt khi n√†o c·∫ßn t·∫£i l·∫°i
    db_mtime = os.path.getmtime(db_path) if os.path.exists(db_path) else 0
    from utils.face_utils import FaceRecognizer
    recognizer = FaceRecognizer("models/face_recognition_sface_2021dec.onnx", db_path)
    print(f"Loaded face recognizer with database timestamp: {db_mtime}")
    return recognizer

def draw_results(frame: np.ndarray, faces: np.ndarray, names: List[str], 
                scores: List[float]) -> np.ndarray:
    """
    Draw detection and recognition results on the frame - simplified version
    Args:
        frame: Input image frame
        faces: Detected faces
        names: Recognized names
        scores: Recognition scores
    Returns:
        frame: Frame with results drawn
    """
    result = frame.copy()
    
    # Draw each face with name and score
    for i, face in enumerate(faces):
        # Skip if index out of range
        if i >= len(names) or i >= len(scores):
            continue
            
        x, y, w, h = map(int, face[:4])
        
        # Get name and score
        name = names[i]
        score = scores[i]
        
        # Choose color based on recognition status
        if name == "Unknown":
            color = (0, 0, 255)  # Red for unknown
            text_color = (255, 255, 255)  # White text
        else:
            color = (0, 255, 0)  # Green for known
            text_color = (255, 255, 255)  # White text
        
        # X√°c ƒë·ªãnh ƒë·ªô d√†y ƒë∆∞·ªùng vi·ªÅn khung v√† c·ª° ch·ªØ nh·∫•t qu√°n
        box_thickness = 2
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6  # C·ªë ƒë·ªãnh font size ƒë·ªÉ nh·∫•t qu√°n
        text_thickness = 2  # C·ªë ƒë·ªãnh ƒë·ªô d√†y vƒÉn b·∫£n
        
        # Draw face rectangle v·ªõi ƒë·ªô d√†y c·ªë ƒë·ªãnh
        cv2.rectangle(result, (x, y), (x+w, y+h), color, box_thickness)
        
        # Prepare text to display
        label = f"{name} ({score:.2f})"
        
        # Calculate text size v·ªõi th√¥ng s·ªë c·ªë ƒë·ªãnh
        (text_width, text_height), baseline = cv2.getTextSize(label, font, font_scale, text_thickness)
        
        # T√≠nh to√°n v·ªã tr√≠ text - ƒë·∫∑t ph√≠a tr√™n khung
        margin = 5  # Margin gi·ªØa text v√† khung
        text_x = x
        text_y = max(y - margin, text_height + baseline)  # ƒê·∫£m b·∫£o text kh√¥ng b·ªã c·∫Øt
        
        # Draw text background t√°ch bi·ªát v·ªõi khung
        cv2.rectangle(result, 
                     (text_x, text_y - text_height - baseline - margin),
                     (text_x + text_width + margin, text_y),
                     color, 
                     cv2.FILLED)
        
        # Draw text v·ªõi ƒë·ªô d√†y c·ªë ƒë·ªãnh
        cv2.putText(result, 
                    label, 
                    (text_x + margin//2, text_y - margin),
                    font, 
                    font_scale, 
                    text_color, 
                    text_thickness)
    
    return result

def show():
    # Th√™m ph·∫ßn gi·ªõi thi·ªáu v√† h∆∞·ªõng d·∫´n
    with st.expander("üîç Gi·ªõi thi·ªáu v·ªÅ nh·∫≠n d·∫°ng khu√¥n m·∫∑t", expanded=False):
        st.markdown("""
        ### Gi·ªõi thi·ªáu v·ªÅ nh·∫≠n d·∫°ng khu√¥n m·∫∑t
        
        T√≠nh nƒÉng nh·∫≠n d·∫°ng khu√¥n m·∫∑t trong ·ª©ng d·ª•ng n√†y s·ª≠ d·ª•ng ki·∫øn tr√∫c hai giai ƒëo·∫°n hi·ªán ƒë·∫°i:
        
        1. **YuNet Face Detector**: M√¥ h√¨nh ph√°t hi·ªán khu√¥n m·∫∑t d·ª±a tr√™n CNN (Convolutional Neural Network) ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a
           - Ki·∫øn tr√∫c: Lightweight network v·ªõi c√°c kh·ªëi ResNet c·∫£i ti·∫øn
           - Hi·ªáu su·∫•t: ƒê·ªô ch√≠nh x√°c cao v√† x·ª≠ l√Ω th·ªùi gian th·ª±c tr√™n CPU
           - ƒê·ªãnh d·∫°ng: ONNX v·ªõi k√≠ch th∆∞·ªõc nh·ªè (< 1MB)
        
        2. **SFace Recognition**: M√¥ h√¨nh nh·∫≠n d·∫°ng khu√¥n m·∫∑t v·ªõi ƒë·ªô ch√≠nh x√°c cao
           - Ki·∫øn tr√∫c: D·ª±a tr√™n m·∫°ng ResNet-50 c·∫£i ti·∫øn cho c√°c embedding vector 128 chi·ªÅu
           - Training: ƒê∆∞·ª£c hu·∫•n luy·ªán tr√™n dataset l·ªõn v·ªõi loss function ƒë·∫∑c bi·ªát gi√∫p ph√¢n bi·ªát t·ªët c√°c khu√¥n m·∫∑t
           - Ph∆∞∆°ng ph√°p: So s√°nh vector embedding v·ªõi c√°c vector trong c∆° s·ªü d·ªØ li·ªáu
        
        **C√°c t√≠nh nƒÉng ch√≠nh:**
        - Nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ ·∫£nh tƒ©nh (t·∫£i l√™n ho·∫∑c ch·ª•p t·ª´ webcam)
        - Nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ video t·∫£i l√™n (nhi·ªÅu ƒë·ªãnh d·∫°ng nh∆∞ MP4, AVI, MOV, v.v.)
        - Ph√°t hi·ªán v√† nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ webcam theo th·ªùi gian th·ª±c
        
        **·ª®ng d·ª•ng th·ª±c t·∫ø:**
        - H·ªá th·ªëng b·∫£o m·∫≠t v√† ki·ªÉm so√°t truy c·∫≠p
        - H·ªá th·ªëng ƒëi·ªÉm danh t·ª± ƒë·ªông
        - Ph√¢n t√≠ch video gi√°m s√°t
        - Nh·∫≠n d·∫°ng khu√¥n m·∫∑t trong d·ªØ li·ªáu h√¨nh ·∫£nh v√† video l·ªõn
        - Tr·∫£i nghi·ªám c√° nh√¢n h√≥a trong c√°c h·ªá th·ªëng th√¥ng minh
        - X√°c th·ª±c danh t√≠nh kh√¥ng ti·∫øp x√∫c
        """)
            
    with st.expander("üìã H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng", expanded=False):
        st.markdown("""
        ### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng
        
        #### 1. Ch·∫ø ƒë·ªô ·∫£nh tƒ©nh
        - **Upload ·∫£nh**: T·∫£i l√™n ·∫£nh ch·ª©a khu√¥n m·∫∑t c·∫ßn nh·∫≠n d·∫°ng
        - **Ch·ª•p t·ª´ webcam**: Ch·ª•p ·∫£nh tr·ª±c ti·∫øp t·ª´ webcam ƒë·ªÉ nh·∫≠n d·∫°ng
        
        #### 2. Ch·∫ø ƒë·ªô video t·∫£i l√™n
        - **Upload video**: T·∫£i l√™n video c√≥ ƒë·ªãnh d·∫°ng MP4, AVI, MOV, MKV, v.v.
        - **ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô x·ª≠ l√Ω**: Ch·ªçn t·ªëc ƒë·ªô x·ª≠ l√Ω (Ch·∫ø ƒë·ªô nhanh / Ch·∫ø ƒë·ªô ch·∫•t l∆∞·ª£ng cao)
        - **T√πy ch·ªçn hi·ªÉn th·ªã**: Hi·ªÉn th·ªã k·∫øt qu·∫£ theo t·ª´ng frame ho·∫∑c video ho√†n ch·ªânh
        - **Thanh ƒëi·ªÅu khi·ªÉn video**: T·∫°m d·ª´ng, tua ƒëi, tua l·∫°i, v.v.
        
        #### 3. Ch·∫ø ƒë·ªô video tr·ª±c ti·∫øp
        - **B·∫Øt ƒë·∫ßu**: M·ªü camera v√† b·∫Øt ƒë·∫ßu ph√°t hi·ªán khu√¥n m·∫∑t
        - **D·ª´ng**: D·ª´ng qu√° tr√¨nh nh·∫≠n d·∫°ng v√† ƒë√≥ng camera
        - **ƒêi·ªÅu ch·ªânh ƒë·ªô ph√¢n gi·∫£i**: Ch·ªçn ƒë·ªô ph√¢n gi·∫£i camera ph√π h·ª£p
        - **T·ªëc ƒë·ªô x·ª≠ l√Ω**: ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô x·ª≠ l√Ω khung h√¨nh (gi√° tr·ªã th·∫•p h∆°n = x·ª≠ l√Ω nhi·ªÅu frame h∆°n)
        
        #### M·∫πo s·ª≠ d·ª•ng:
        - **√Ånh s√°ng**: ƒê·∫£m b·∫£o khu√¥n m·∫∑t ƒë∆∞·ª£c chi·∫øu s√°ng t·ªët
        - **G√≥c nh√¨n**: N√™n ch·ªçn g√≥c th·∫≥ng ho·∫∑c nghi√™ng nh·∫π ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët nh·∫•t
        - **Kho·∫£ng c√°ch**: Khu√¥n m·∫∑t n√™n chi·∫øm kho·∫£ng 10-15% khung h√¨nh ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªëi ∆∞u
        - **Ch·∫•t l∆∞·ª£ng video**: ∆Øu ti√™n video c√≥ ƒë·ªô ph√¢n gi·∫£i cao (720p tr·ªü l√™n) v√† √≠t nhi·ªÖu
        - **ƒêƒÉng k√Ω m·∫∑t m·ªõi**: N·∫øu khu√¥n m·∫∑t ch∆∞a ƒë∆∞·ª£c nh·∫≠n d·∫°ng, s·ª≠ d·ª•ng ch·ª©c nƒÉng "ƒêƒÉng k√Ω khu√¥n m·∫∑t m·ªõi"
        
        #### X·ª≠ l√Ω l·ªói:
        - **Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c**: ƒê·∫£m b·∫£o khu√¥n m·∫∑t ƒë√£ ƒë∆∞·ª£c ƒëƒÉng k√Ω trong h·ªá th·ªëng
        - **Nh·∫≠n d·∫°ng sai**: C·∫≠p nh·∫≠t database v·ªõi nhi·ªÅu m·∫´u khu√¥n m·∫∑t h∆°n
        - **L·ªói x·ª≠ l√Ω video**: Th·ª≠ chuy·ªÉn ƒë·ªïi video sang MP4 ho·∫∑c gi·∫£m ƒë·ªô ph√¢n gi·∫£i
        - **Kh√¥ng hi·ªÉn th·ªã camera**: Ki·ªÉm tra quy·ªÅn truy c·∫≠p camera trong tr√¨nh duy·ªát
        """)
    
    # Load models
    face_detector = load_face_detector()
    face_recognizer = load_face_recognizer()
    
    # Choose input mode
    mode = st.radio("Ch·ªçn ch·∫ø ƒë·ªô nh·∫≠n d·∫°ng:", ["üì∏ ·∫¢nh tƒ©nh", "üé¨ Video t·∫£i l√™n", "üé• Video tr·ª±c ti·∫øp"])
    
    # Process based on selected mode
    if mode == "üì∏ ·∫¢nh tƒ©nh":
        # Image input methods
        input_method = st.radio("Ch·ªçn ngu·ªìn ·∫£nh:", ["Upload ·∫£nh", "Ch·ª•p t·ª´ webcam"])
        
        if input_method == "Ch·ª•p t·ª´ webcam":
            # Camera capture interface  
            picture = st.camera_input("Ch·ª•p ·∫£nh khu√¥n m·∫∑t")
            
            if picture:
                # Convert to OpenCV format
                bytes_data = picture.getvalue()
                img = cv2.imdecode(np.frombuffer(bytes_data, np.uint8), cv2.IMREAD_COLOR)
                
                # Create two columns for original and result images
                col1, col2 = st.columns(2)
                
                # Display original image
                with col1:
                    st.subheader("·∫¢nh g·ªëc")
                    st.image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), use_container_width=True)
                
                # Process image
                with st.spinner("ƒêang nh·∫≠n d·∫°ng khu√¥n m·∫∑t..."):
                    faces, aligned_faces = face_detector.detect(img)
                    
                    if len(faces) > 0:
                        # S·∫Øp x·∫øp khu√¥n m·∫∑t theo k√≠ch th∆∞·ªõc (l·ªõn -> nh·ªè)
                        face_sizes = [f[2] * f[3] for f in faces]  # width * height
                        sorted_indices = np.argsort(face_sizes)[::-1]  # Gi·∫£m d·∫ßn
                        
                        sorted_faces = faces[sorted_indices]
                        sorted_aligned_faces = [aligned_faces[i] for i in sorted_indices if i < len(aligned_faces)]
                        
                        names = []
                        scores = []
                        
                        # Nh·∫≠n di·ªán khu√¥n m·∫∑t (limit to 5)
                        for face_img in sorted_aligned_faces[:5]:
                            # Ch·ªâ s·ª≠ d·ª•ng ·∫£nh g·ªëc, lo·∫°i b·ªè ph∆∞∆°ng ph√°p flip
                            name, score = face_recognizer.identify(face_img, threshold=0.45)
                            names.append(name)
                            scores.append(score)
                        
                        # Draw results - s·ª≠ d·ª•ng h√†m ƒë∆°n gi·∫£n h√≥a
                        result_img = draw_results(img, sorted_faces[:5], names, scores)
                    else:
                        result_img = img  # No faces detected
                        names = []
                        scores = []
                    
                    # Display result
                    with col2:
                        st.subheader("K·∫øt qu·∫£")
                        st.image(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB), use_container_width=True)
                    
                    # Display information below images
                    if names:
                        st.markdown("### Th√¥ng tin nh·∫≠n d·∫°ng:")
                        for i, (name, score) in enumerate(zip(names, scores)):
                            status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if name != "Unknown" else "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c"
                            st.write(f"**Ng∆∞·ªùi {i+1}:** {name} ({score:.2f}) - {status}")
                    else:
                        st.warning("Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t n√†o trong ·∫£nh!")
        
        else:  # Upload image mode
            uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh ch·ª©a khu√¥n m·∫∑t", type=["jpg", "jpeg", "png"])
            
            if uploaded_file is not None:
                # Read image
                file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
                img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
                
                # Create two columns for original and result images
                col1, col2 = st.columns(2)
                
                # Display original image
                with col1:
                    st.subheader("·∫¢nh g·ªëc")
                    st.image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), use_container_width=True)
                
                # Process image
                with st.spinner("ƒêang nh·∫≠n d·∫°ng khu√¥n m·∫∑t..."):
                    faces, aligned_faces = face_detector.detect(img)
                    
                    if len(faces) > 0:
                        # S·∫Øp x·∫øp khu√¥n m·∫∑t theo k√≠ch th∆∞·ªõc (l·ªõn -> nh·ªè)
                        face_sizes = [f[2] * f[3] for f in faces]  # width * height
                        sorted_indices = np.argsort(face_sizes)[::-1]  # Gi·∫£m d·∫ßn
                        
                        sorted_faces = faces[sorted_indices]
                        sorted_aligned_faces = [aligned_faces[i] for i in sorted_indices if i < len(aligned_faces)]
                        
                        names = []
                        scores = []
                        
                        # Nh·∫≠n di·ªán khu√¥n m·∫∑t (limit to 5)
                        for face_img in sorted_aligned_faces[:5]:
                            # Ch·ªâ s·ª≠ d·ª•ng ·∫£nh g·ªëc, lo·∫°i b·ªè ph∆∞∆°ng ph√°p flip
                            name, score = face_recognizer.identify(face_img, threshold=0.45)
                            names.append(name)
                            scores.append(score)
                        
                        # Draw results - s·ª≠ d·ª•ng h√†m ƒë∆°n gi·∫£n h√≥a
                        result_img = draw_results(img, sorted_faces[:5], names, scores)
                    else:
                        result_img = img  # No faces detected
                        names = []
                        scores = []
                    
                    # Display result
                    with col2:
                        st.subheader("K·∫øt qu·∫£")
                        st.image(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB), use_container_width=True)
                    
                    # Display information below images
                    if names:
                        st.markdown("### Th√¥ng tin nh·∫≠n d·∫°ng:")
                        for i, (name, score) in enumerate(zip(names, scores)):
                            status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if name != "Unknown" else "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c"
                            st.write(f"**Ng∆∞·ªùi {i+1}:** {name} ({score:.2f}) - {status}")
                    else:
                        st.warning("Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t n√†o trong ·∫£nh!")
        
    elif mode == "üé¨ Video t·∫£i l√™n":
        # Video upload section with improved explanation
        st.subheader("Nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ video")
        
        # Add helpful explanation
        st.markdown("""
        T√≠nh nƒÉng n√†y cho ph√©p nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ file video t·∫£i l√™n.
        Qu√° tr√¨nh x·ª≠ l√Ω s·∫Ω di·ªÖn ra theo th·ªùi gian th·ª±c - b·∫°n s·∫Ω th·∫•y k·∫øt qu·∫£ ngay khi video ƒëang ch·∫°y!
        """)
        
        # Upload video file with clearer instructions
        video_file = st.file_uploader(
            "T·∫£i l√™n video ch·ª©a khu√¥n m·∫∑t c·∫ßn nh·∫≠n d·∫°ng", 
            type=["mp4", "mov", "avi", "mkv", "wmv"],
            help="H·ªó tr·ª£ c√°c ƒë·ªãnh d·∫°ng: MP4, MOV, AVI, MKV, WMV. N√™n s·ª≠ d·ª•ng video MP4 ƒë·ªÉ c√≥ hi·ªáu su·∫•t t·ªët nh·∫•t."
        )
        
        if video_file is not None:
            # Process video in realtime mode
            process_video_realtime(video_file, face_detector, face_recognizer)
            
    elif mode == "üé• Video tr·ª±c ti·∫øp":
        st.markdown("### üé• Video tr·ª±c ti·∫øp t·ª´ camera")
        
        # Performance settings
        col1, col2 = st.columns(2)
        with col1:
            resolution = st.selectbox("ƒê·ªô ph√¢n gi·∫£i:", 
                                    ["640x480", "800x600", "1280x720"],
                                    index=0)
        with col2:
            process_rate = st.slider("T·ªëc ƒë·ªô x·ª≠ l√Ω:", 1, 6, 2, 1,
                                    help="S·ªë frame b·ªè qua gi·ªØa c√°c l·∫ßn x·ª≠ l√Ω (cao h∆°n = nhanh h∆°n)")
        
        # Parse resolution
        w, h = map(int, resolution.split('x'))
        
        # Control buttons
        col1, col2 = st.columns([1, 4])
        with col1:
            start_button = st.button("B·∫Øt ƒë·∫ßu", type="primary", use_container_width=True)
        with col2:
            stop_button = st.button("D·ª´ng", use_container_width=True, key="stop_button")
        
        # Initialize session state
        if 'video_running' not in st.session_state:
            st.session_state.video_running = False
        
        # Video display area
        video_placeholder = st.empty()
        info_placeholder = st.empty()
        
        # Start video stream
        if start_button:
            st.session_state.video_running = True
        
        if stop_button:
            st.session_state.video_running = False
        
        # Video processing loop
        if st.session_state.video_running:
            # Hi·ªÉn th·ªã loading state
            video_placeholder.markdown("### ‚è≥ ƒêang m·ªü camera...")
            
            # Try different camera indices
            cap = None
            for idx in range(3):  # Try index 0, 1, 2
                cap = cv2.VideoCapture(idx)
                if cap.isOpened():
                    break
                cap.release()
            
            if cap is None or not cap.isOpened():
                st.error("Kh√¥ng th·ªÉ m·ªü camera. Vui l√≤ng ki·ªÉm tra l·∫°i k·∫øt n·ªëi camera.")
            else:
                # Set camera resolution
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, w)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, h)
                cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                
                # Hi·ªÉn th·ªã th√†nh c√¥ng
                video_placeholder.markdown("‚úÖ Camera ƒë√£ s·∫µn s√†ng")
                time.sleep(0.5)  # Hi·ªÉn th·ªã th√¥ng b√°o trong 0.5 gi√¢y
                
                # FPS counter
                fps_start_time = time.time()
                fps_counter = 0
                current_fps = 0
                frame_num = 0
                
                while st.session_state.video_running:
                    ret, frame = cap.read()
                    
                    if not ret:
                        st.error("Kh√¥ng th·ªÉ ƒë·ªçc khung h√¨nh t·ª´ camera!")
                        break
                    
                    # Flip frame horizontally for mirror effect
                    frame = cv2.flip(frame, 1)
                    
                    # Process every Nth frame based on speed setting
                    if frame_num % process_rate == 0:
                        # Detect faces
                        faces, aligned_faces = face_detector.detect(frame)
                        
                        # N·∫øu ph√°t hi·ªán c√°c khu√¥n m·∫∑t, s·∫Øp x·∫øp ch√∫ng theo k√≠ch th∆∞·ªõc
                        if len(faces) > 0:
                            face_sizes = [f[2] * f[3] for f in faces]  # width * height
                            sorted_indices = np.argsort(face_sizes)[::-1]  # Gi·∫£m d·∫ßn
                            
                            sorted_faces = faces[sorted_indices]
                            sorted_aligned_faces = [aligned_faces[i] for i in sorted_indices if i < len(aligned_faces)]
                            
                            # Nh·∫≠n di·ªán khu√¥n m·∫∑t
                            names = []
                            scores = []
                            
                            for face_img in sorted_aligned_faces[:5]:  # Ch·ªâ x·ª≠ l√Ω t·ªëi ƒëa 5 khu√¥n m·∫∑t
                                # Ch·ªâ s·ª≠ d·ª•ng ·∫£nh g·ªëc, lo·∫°i b·ªè ph∆∞∆°ng ph√°p flip
                                name, score = face_recognizer.identify(face_img, threshold=0.45)
                                names.append(name)
                                scores.append(score)
                            
                            # V·∫Ω k·∫øt qu·∫£
                            display_frame = draw_results(frame, sorted_faces, names, scores)
                            
                            # Display recognition info
                            if len(names) > 0:
                                info_text = "### Khu√¥n m·∫∑t ƒë∆∞·ª£c nh·∫≠n di·ªán:\n\n"
                                
                                for i, (name, score) in enumerate(zip(names, scores)):
                                    status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if name != "Unknown" else "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c"
                                    info_text += f"**Ng∆∞·ªùi {i+1}:** {name} ({score:.2f}) - {status}\n\n"
                                
                                info_placeholder.markdown(info_text)
                        else:
                            display_frame = frame
                            if frame_num % (process_rate * 5) == 0:
                                info_placeholder.markdown("*Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t n√†o*")
                    else:
                        display_frame = frame
                    
                    frame_num += 1
                    
                    # Calculate FPS
                    fps_counter += 1
                    if time.time() - fps_start_time >= 1.0:
                        current_fps = fps_counter
                        fps_counter = 0
                        fps_start_time = time.time()
                    
                    # Add FPS text to frame
                    cv2.putText(display_frame, f"FPS: {current_fps}", (10, 30),
                              cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                    
                    # Display frame
                    video_placeholder.image(cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB), 
                                          channels="RGB", use_container_width=True)
                    
                    # Check if stop button was pressed
                    if not st.session_state.video_running:
                        break
                
                # Release camera
                cap.release()
                video_placeholder.markdown("**Camera ƒë√£ ƒë∆∞·ª£c ƒë√≥ng**")
        
        elif not st.session_state.video_running and not start_button:
            video_placeholder.markdown("""
            ## üé• Ch·∫ø ƒë·ªô Video Tr·ª±c Ti·∫øp
            
            **T√≠nh nƒÉng:**
            - Hi·ªÉn th·ªã khu√¥n m·∫∑t trong th·ªùi gian th·ª±c
            - T·ª± ƒë·ªông nh·∫≠n d·∫°ng khu√¥n m·∫∑t trong th·ªùi gian th·ª±c
            - Hi·ªÉn th·ªã FPS ƒë·ªÉ theo d√µi hi·ªáu su·∫•t
            
            **T√πy ch·ªçn hi·ªáu su·∫•t:**
            - ƒê·ªô ph√¢n gi·∫£i th·∫•p = nhanh h∆°n
            - T·ªëc ƒë·ªô x·ª≠ l√Ω cao = x·ª≠ l√Ω nhi·ªÅu frame h∆°n/gi√¢y
            
            Nh·∫•n **B·∫Øt ƒë·∫ßu** ƒë·ªÉ m·ªü camera.
            """)
            
def process_video_realtime(video_file, face_detector, face_recognizer):
    """
    X·ª≠ l√Ω video theo th·ªùi gian th·ª±c - kh√¥ng c·∫ßn ƒë·ª£i x·ª≠ l√Ω t·∫•t c·∫£ c√°c frame
    """
    import tempfile
    import os
    import threading
    import queue
    import time
    from datetime import timedelta
    
    # T·∫°o file t·∫°m
    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmpfile:
        temp_video_path = tmpfile.name
        # L∆∞u video v√†o file t·∫°m
        tmpfile.write(video_file.getbuffer())
    
    # M·ªü video
    cap = cv2.VideoCapture(temp_video_path)
    if not cap.isOpened():
        st.error("Kh√¥ng th·ªÉ ƒë·ªçc video. Vui l√≤ng th·ª≠ v·ªõi ƒë·ªãnh d·∫°ng kh√°c.")
        try:
            os.remove(temp_video_path)
        except:
            pass
        return
    
    # L·∫•y th√¥ng tin video
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    duration = frame_count / fps if fps > 0 else 0
    
    # Hi·ªÉn th·ªã th√¥ng tin video
    st.info(f"Th√¥ng tin video: {frame_width}x{frame_height}, {fps:.1f} FPS, {frame_count} frames, th·ªùi l∆∞·ª£ng: {timedelta(seconds=duration)}")
    
    # ƒêi·ªÅu ch·ªânh ƒë·ªô nh·∫°y v√† t·ªëc ƒë·ªô x·ª≠ l√Ω
    col1, col2 = st.columns(2)
    with col1:
        confidence_threshold = st.slider(
            "Ng∆∞·ª°ng nh·∫≠n di·ªán:", 
            min_value=0.4, 
            max_value=0.95, 
            value=0.45, 
            step=0.05,
            help="ƒêi·ªÅu ch·ªânh ƒë·ªô nh·∫°y khi nh·∫≠n di·ªán (cao h∆°n = √≠t nh·∫≠n di·ªán sai h∆°n)"
        )
    with col2:
        skip_frames = st.slider(
            "T·ªëc ƒë·ªô x·ª≠ l√Ω:", 
            min_value=1, 
            max_value=10, 
            value=2, 
            step=1,
            help="S·ªë frame b·ªè qua khi x·ª≠ l√Ω (cao h∆°n = nhanh h∆°n nh∆∞ng m·∫•t m·ªôt s·ªë chi ti·∫øt)"
        )
    
    # Th√¥ng tin theo d√µi
    stats = {
        'total_faces': 0,
        'identified_faces': 0,
        'unknown_faces': 0,
        'people_detected': set(),
        'people_frames': {}
    }
    
    # Queue ƒë·ªÉ trao ƒë·ªïi d·ªØ li·ªáu gi·ªØa c√°c lu·ªìng
    frame_queue = queue.Queue(maxsize=30)  # Buffer 30 frames
    result_queue = queue.Queue(maxsize=30)
    stop_event = threading.Event()
    
    # Placeholder hi·ªÉn th·ªã
    video_placeholder = st.empty()
    progress_bar = st.progress(0.0)
    info_placeholder = st.empty()
    stats_placeholder = st.empty()
    
    # Thread x·ª≠ l√Ω frame
    def process_frames():
        frame_idx = 0
        processed_idx = 0
        
        while not stop_event.is_set():
            try:
                if frame_queue.empty():
                    time.sleep(0.01)
                    continue
                
                frame, current_pos = frame_queue.get()
                frame_idx += 1
                
                # Ch·ªâ x·ª≠ l√Ω m·ªói N frame theo skip_frames
                if frame_idx % skip_frames == 0:
                    # Face detection
                    start_time = time.time()
                    faces, aligned_faces = face_detector.detect(frame)
                    
                    # S·∫Øp x·∫øp khu√¥n m·∫∑t theo k√≠ch th∆∞·ªõc (l·ªõn -> nh·ªè)
                    if len(faces) > 0:
                        face_sizes = [f[2] * f[3] for f in faces]  # width * height
                        sorted_indices = np.argsort(face_sizes)[::-1]  # Gi·∫£m d·∫ßn
                        
                        sorted_faces = faces[sorted_indices]
                        sorted_aligned_faces = [aligned_faces[i] for i in sorted_indices if i < len(aligned_faces)]
                        
                        names = []
                        scores = []
                        
                        stats['total_faces'] += len(faces)
                        
                        # Nh·∫≠n di·ªán khu√¥n m·∫∑t (t·ªëi ƒëa 5 khu√¥n m·∫∑t)
                        for face_img in sorted_aligned_faces[:5]:
                            # Ch·ªâ s·ª≠ d·ª•ng ·∫£nh g·ªëc, kh√¥ng c√≤n ph∆∞∆°ng ph√°p flip
                            name, score = face_recognizer.identify(face_img, threshold=confidence_threshold)
                            
                            # C·∫≠p nh·∫≠t th·ªëng k√™
                            if name != "Unknown":
                                stats['identified_faces'] += 1
                                stats['people_detected'].add(name)
                                stats['people_frames'][name] = stats['people_frames'].get(name, 0) + 1
                            else:
                                stats['unknown_faces'] += 1
                                
                            names.append(name)
                            scores.append(score)
                        
                        # V·∫Ω k·∫øt qu·∫£
                        result_frame = draw_results(frame, sorted_faces, names, scores)
                    else:
                        result_frame = frame
                        names = []
                        scores = []
                    
                    # T√≠nh th·ªùi gian x·ª≠ l√Ω
                    process_time = time.time() - start_time
                    processing_fps = 1/process_time if process_time > 0 else 0
                    
                    # Th√™m th√¥ng tin v√†o khung h√¨nh
                    cv2.putText(
                        result_frame, 
                        f"Processing: {processing_fps:.1f} FPS", 
                        (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2
                    )
                    
                    # Th√™m v·ªã tr√≠ frame
                    position_text = f"Frame: {frame_idx}/{frame_count} ({current_pos*100:.0f}%)"
                    cv2.putText(
                        result_frame, 
                        position_text, 
                        (10, frame_height - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2
                    )
                    
                    # G·ª≠i k·∫øt qu·∫£ v√†o queue
                    result_queue.put((result_frame, names, scores, current_pos))
                    processed_idx += 1
                
                frame_queue.task_done()
                
            except Exception as e:
                print(f"L·ªói x·ª≠ l√Ω frame: {e}")
                if frame_queue.qsize() > 0:
                    frame_queue.task_done()
                time.sleep(0.1)
    
    # Thread ƒë·ªçc frame
    def read_frames():
        frame_idx = 0
        try:
            while cap.isOpened() and not stop_event.is_set():
                ret, frame = cap.read()
                if not ret:
                    break
                
                # T√≠nh v·ªã tr√≠ t∆∞∆°ng ƒë·ªëi
                current_pos = cap.get(cv2.CAP_PROP_POS_FRAMES) / frame_count
                
                # ƒê∆∞a frame v√†o queue ƒë·ªÉ x·ª≠ l√Ω, v·ªõi ki·ªÉm tra ƒë·ªÉ tr√°nh t·∫Øc ngh·∫Ωn
                if not frame_queue.full():
                    frame_queue.put((frame, current_pos))
                else:
                    # N·∫øu queue ƒë·∫ßy, ƒë·ª£i m·ªôt ch√∫t
                    time.sleep(0.01)
                
                frame_idx += 1
                
        except Exception as e:
            print(f"L·ªói ƒë·ªçc frame: {e}")
        finally:
            # ƒê√°nh d·∫•u ƒë√£ ƒë·ªçc xong
            stop_event.set()
            cap.release()
    
    # N√∫t ƒëi·ªÅu khi·ªÉn
    st.markdown("### ƒêi·ªÅu khi·ªÉn x·ª≠ l√Ω video")
    col1, col2 = st.columns(2)
    
    with col1:
        start_button = st.button("‚ñ∂Ô∏è B·∫Øt ƒë·∫ßu x·ª≠ l√Ω", type="primary", use_container_width=True)
    with col2:
        stop_button = st.button("‚èπÔ∏è D·ª´ng x·ª≠ l√Ω", use_container_width=True)
    
    if start_button:
        # Kh·ªüi ƒë·ªông c√°c thread
        process_thread = threading.Thread(target=process_frames)
        read_thread = threading.Thread(target=read_frames)
        
        process_thread.daemon = True
        read_thread.daemon = True
        
        # B·∫Øt ƒë·∫ßu x·ª≠ l√Ω
        process_thread.start()
        read_thread.start()
        
        start_time = time.time()
        frame_count_displayed = 0
        
        # Hi·ªÉn th·ªã k·∫øt qu·∫£ theo th·ªùi gian th·ª±c
        st.markdown("### Video ƒëang x·ª≠ l√Ω")
        
        # V√≤ng l·∫∑p hi·ªÉn th·ªã k·∫øt qu·∫£
        try:
            while not stop_event.is_set() or not result_queue.empty():
                if stop_button:
                    stop_event.set()
                    st.warning("ƒêang d·ª´ng x·ª≠ l√Ω...")
                    break
                
                if not result_queue.empty():
                    result_frame, names, scores, pos = result_queue.get()
                    frame_count_displayed += 1
                    
                    # C·∫≠p nh·∫≠t video v√† thanh ti·∫øn tr√¨nh
                    progress_bar.progress(pos)
                    video_placeholder.image(
                        cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB),
                        use_container_width=True
                    )
                    
                    # T√≠nh FPS hi·ªÉn th·ªã
                    elapsed = time.time() - start_time
                    display_fps = frame_count_displayed / elapsed if elapsed > 0 else 0
                    
                    # Hi·ªÉn th·ªã th√¥ng tin nh·∫≠n di·ªán hi·ªán t·∫°i
                    if names:
                        info_text = f"**T·ªëc ƒë·ªô hi·ªÉn th·ªã: {display_fps:.1f} FPS | Khu√¥n m·∫∑t nh·∫≠n di·ªán ƒë∆∞·ª£c:**\n\n"
                        for name, score in zip(names, scores):
                            status = "‚úÖ" if name != "Unknown" else "‚ùå"
                            info_text += f"{status} {name} ({score:.2f}) "
                        info_placeholder.markdown(info_text)
                    
                    # Hi·ªÉn th·ªã th·ªëng k√™ t·ªïng h·ª£p
                    if frame_count_displayed % 10 == 0:  # C·∫≠p nh·∫≠t m·ªói 10 frame
                        stats_text = f"""
                        ### Th·ªëng k√™ x·ª≠ l√Ω:
                        - ƒê√£ x·ª≠ l√Ω: {frame_count_displayed} frames ({display_fps:.1f} FPS)
                        - Khu√¥n m·∫∑t ph√°t hi·ªán: {stats['total_faces']}
                        - Khu√¥n m·∫∑t nh·∫≠n di·ªán: {stats['identified_faces']}
                        - Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c: {stats['unknown_faces']}
                        - S·ªë ng∆∞·ªùi nh·∫≠n di·ªán ƒë∆∞·ª£c: {len(stats['people_detected'])}
                        """
                        
                        # Hi·ªÉn th·ªã top 3 ng∆∞·ªùi xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
                        if stats['people_frames']:
                            stats_text += "\n\n**Top ng∆∞·ªùi xu·∫•t hi·ªán nhi·ªÅu nh·∫•t:**\n"
                            sorted_people = sorted(stats['people_frames'].items(), key=lambda x: x[1], reverse=True)
                            for person, count in sorted_people[:3]:
                                stats_text += f"- {person}: {count} frames\n"
                                
                        stats_placeholder.markdown(stats_text)
                    
                    result_queue.task_done()
                else:
                    # N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£ m·ªõi, ƒë·ª£i m·ªôt ch√∫t
                    time.sleep(0.01)
        
        except Exception as e:
            st.error(f"L·ªói hi·ªÉn th·ªã k·∫øt qu·∫£: {e}")
        finally:
            # ƒê·∫£m b·∫£o d·ª´ng t·∫•t c·∫£ lu·ªìng
            stop_event.set()
            
            # ƒê·ª£i c√°c thread k·∫øt th√∫c
            if 'process_thread' in locals() and process_thread.is_alive():
                process_thread.join(timeout=1.0)
            if 'read_thread' in locals() and read_thread.is_alive():
                read_thread.join(timeout=1.0)
            
            # T√≠nh th·ªùi gian ch·∫°y
            run_time = time.time() - start_time
            
            # Th√¥ng b√°o k·∫øt th√∫c
            st.success(f"ƒê√£ ho√†n th√†nh x·ª≠ l√Ω {frame_count_displayed} frames trong {run_time:.1f} gi√¢y!")
            
            # Hi·ªÉn th·ªã th·ªëng k√™ cu·ªëi c√πng
            st.subheader("K·∫øt qu·∫£ nh·∫≠n di·ªán")
            
            # Hi·ªÉn th·ªã c√°c ng∆∞·ªùi ƒë∆∞·ª£c nh·∫≠n di·ªán 
            if stats['people_detected']:
                st.markdown("### Ng∆∞·ªùi xu·∫•t hi·ªán trong video:")
                
                # S·∫Øp x·∫øp theo s·ªë l·∫ßn xu·∫•t hi·ªán
                if stats['people_frames']:
                    people_cols = st.columns(min(3, len(stats['people_frames'])))
                    sorted_people = sorted(stats['people_frames'].items(), key=lambda x: x[1], reverse=True)
                    
                    for i, (person, frames) in enumerate(sorted_people):
                        with people_cols[i % 3]:
                            frame_percent = frames / frame_count_displayed * 100
                            st.metric(
                                label=person,
                                value=f"{frames} frames",
                                delta=f"{frame_percent:.1f}%"
                            )
            else:
                st.warning("Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c ng∆∞·ªùi n√†o trong video.")
    
    # D·ªçn d·∫πp
    try:
        os.remove(temp_video_path)
    except:
        pass