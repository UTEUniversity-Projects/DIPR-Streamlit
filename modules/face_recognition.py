import streamlit as st
import cv2
import numpy as np
import time
import os
from typing import List, Dict, Tuple, Optional

@st.cache_resource
def load_face_detector():
    from utils.face_utils import FaceDetector
    return FaceDetector("models/face_detection_yunet_2023mar.onnx")

@st.cache_resource(ttl=10)  # Time to live: ch·ªâ 10 gi√¢y ƒë·ªÉ t·∫£i l·∫°i nhanh h∆°n
def load_face_recognizer():
    db_path = "data/db_embeddings.pkl"
    # Th√™m mtime l√†m tham s·ªë ƒë·ªÉ Streamlit bi·∫øt khi n√†o c·∫ßn t·∫£i l·∫°i
    db_mtime = os.path.getmtime(db_path) if os.path.exists(db_path) else 0
    from utils.face_utils import FaceRecognizer
    recognizer = FaceRecognizer("models/face_recognition_sface_2021dec.onnx", db_path)
    print(f"Loaded face recognizer with database timestamp: {db_mtime}")
    return recognizer

def draw_results(frame: np.ndarray, faces: np.ndarray, names: List[str], 
                scores: List[float]) -> np.ndarray:
    """
    Draw detection and recognition results on the frame - simplified version
    Args:
        frame: Input image frame
        faces: Detected faces
        names: Recognized names
        scores: Recognition scores
    Returns:
        frame: Frame with results drawn
    """
    result = frame.copy()
    
    # Draw each face with name and score
    for i, face in enumerate(faces):
        # Skip if index out of range
        if i >= len(names) or i >= len(scores):
            continue
            
        x, y, w, h = map(int, face[:4])
        
        # Get name and score
        name = names[i]
        score = scores[i]
        
        # Choose color based on recognition status
        if name == "Unknown":
            color = (0, 0, 255)  # Red for unknown
            text_color = (255, 255, 255)  # White text
        else:
            color = (0, 255, 0)  # Green for known
            text_color = (255, 255, 255)  # White text
        
        # X√°c ƒë·ªãnh ƒë·ªô d√†y ƒë∆∞·ªùng vi·ªÅn khung v√† c·ª° ch·ªØ nh·∫•t qu√°n
        box_thickness = 2
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6  # C·ªë ƒë·ªãnh font size ƒë·ªÉ nh·∫•t qu√°n
        text_thickness = 2  # C·ªë ƒë·ªãnh ƒë·ªô d√†y vƒÉn b·∫£n
        
        # Draw face rectangle v·ªõi ƒë·ªô d√†y c·ªë ƒë·ªãnh
        cv2.rectangle(result, (x, y), (x+w, y+h), color, box_thickness)
        
        # Prepare text to display
        label = f"{name} ({score:.2f})"
        
        # Calculate text size v·ªõi th√¥ng s·ªë c·ªë ƒë·ªãnh
        (text_width, text_height), baseline = cv2.getTextSize(label, font, font_scale, text_thickness)
        
        # T√≠nh to√°n v·ªã tr√≠ text - ƒë·∫∑t ph√≠a tr√™n khung
        margin = 5  # Margin gi·ªØa text v√† khung
        text_x = x
        text_y = max(y - margin, text_height + baseline)  # ƒê·∫£m b·∫£o text kh√¥ng b·ªã c·∫Øt
        
        # Draw text background t√°ch bi·ªát v·ªõi khung
        cv2.rectangle(result, 
                     (text_x, text_y - text_height - baseline - margin),
                     (text_x + text_width + margin, text_y),
                     color, 
                     cv2.FILLED)
        
        # Draw text v·ªõi ƒë·ªô d√†y c·ªë ƒë·ªãnh
        cv2.putText(result, 
                    label, 
                    (text_x + margin//2, text_y - margin),
                    font, 
                    font_scale, 
                    text_color, 
                    text_thickness)
    
    return result

def show():
    # Th√™m ph·∫ßn gi·ªõi thi·ªáu v√† h∆∞·ªõng d·∫´n
    with st.expander("üîç Gi·ªõi thi·ªáu v·ªÅ nh·∫≠n d·∫°ng khu√¥n m·∫∑t", expanded=False):
        st.markdown("""
        ### Gi·ªõi thi·ªáu v·ªÅ nh·∫≠n d·∫°ng khu√¥n m·∫∑t
        
        T√≠nh nƒÉng nh·∫≠n d·∫°ng khu√¥n m·∫∑t trong ·ª©ng d·ª•ng n√†y s·ª≠ d·ª•ng ki·∫øn tr√∫c hai giai ƒëo·∫°n hi·ªán ƒë·∫°i:
        
        1. **YuNet Face Detector**: M√¥ h√¨nh ph√°t hi·ªán khu√¥n m·∫∑t d·ª±a tr√™n CNN (Convolutional Neural Network) ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a
           - Ki·∫øn tr√∫c: Lightweight network v·ªõi c√°c kh·ªëi ResNet c·∫£i ti·∫øn
           - Hi·ªáu su·∫•t: ƒê·ªô ch√≠nh x√°c cao v√† x·ª≠ l√Ω th·ªùi gian th·ª±c tr√™n CPU
           - ƒê·ªãnh d·∫°ng: ONNX v·ªõi k√≠ch th∆∞·ªõc nh·ªè (< 1MB)
        
        2. **SFace Recognition**: M√¥ h√¨nh nh·∫≠n d·∫°ng khu√¥n m·∫∑t v·ªõi ƒë·ªô ch√≠nh x√°c cao
           - Ki·∫øn tr√∫c: D·ª±a tr√™n m·∫°ng ResNet-50 c·∫£i ti·∫øn cho c√°c embedding vector 128 chi·ªÅu
           - Training: ƒê∆∞·ª£c hu·∫•n luy·ªán tr√™n dataset l·ªõn v·ªõi loss function ƒë·∫∑c bi·ªát gi√∫p ph√¢n bi·ªát t·ªët c√°c khu√¥n m·∫∑t
           - Ph∆∞∆°ng ph√°p: So s√°nh vector embedding v·ªõi c√°c vector trong c∆° s·ªü d·ªØ li·ªáu
        
        **C√°c t√≠nh nƒÉng ch√≠nh:**
        - Nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ ·∫£nh tƒ©nh (t·∫£i l√™n ho·∫∑c ch·ª•p t·ª´ webcam)
        - Nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ video t·∫£i l√™n (nhi·ªÅu ƒë·ªãnh d·∫°ng nh∆∞ MP4, AVI, MOV, v.v.)
        - Ph√°t hi·ªán v√† nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ webcam theo th·ªùi gian th·ª±c
        """)
            
    with st.expander("üìã H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng", expanded=False):
        st.markdown("""
        ### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng
        
        #### 1. Ch·∫ø ƒë·ªô ·∫£nh tƒ©nh
        - **Upload ·∫£nh**: T·∫£i l√™n ·∫£nh ch·ª©a khu√¥n m·∫∑t c·∫ßn nh·∫≠n d·∫°ng
        - **Ch·ª•p t·ª´ webcam**: Ch·ª•p ·∫£nh tr·ª±c ti·∫øp t·ª´ webcam ƒë·ªÉ nh·∫≠n d·∫°ng
        
        #### 2. Ch·∫ø ƒë·ªô video t·∫£i l√™n
        - **Upload video**: T·∫£i l√™n video c√≥ ƒë·ªãnh d·∫°ng MP4, AVI, MOV, MKV, v.v.
        - **ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô x·ª≠ l√Ω**: Ch·ªçn t·ªëc ƒë·ªô x·ª≠ l√Ω (cao h∆°n = nhanh h∆°n)
        - **ƒêi·ªÅu ch·ªânh ƒë·ªô nh·∫°y**: Thay ƒë·ªïi ng∆∞·ª°ng x√°c ƒë·ªãnh cho k·∫øt qu·∫£ t·ªët h∆°n
        - **Hi·ªÉn th·ªã video**: Xem video v·ªõi khu√¥n m·∫∑t ƒë∆∞·ª£c ƒë√°nh d·∫•u tr·ª±c ti·∫øp
        
        #### 3. Ch·∫ø ƒë·ªô video tr·ª±c ti·∫øp
        - **B·∫Øt ƒë·∫ßu**: M·ªü camera v√† b·∫Øt ƒë·∫ßu ph√°t hi·ªán khu√¥n m·∫∑t
        - **D·ª´ng**: D·ª´ng qu√° tr√¨nh nh·∫≠n d·∫°ng v√† ƒë√≥ng camera
        """)
    
    # Load models
    face_detector = load_face_detector()
    face_recognizer = load_face_recognizer()
    
    # Choose input mode
    mode = st.radio("Ch·ªçn ch·∫ø ƒë·ªô nh·∫≠n d·∫°ng:", ["üì∏ ·∫¢nh tƒ©nh", "üé¨ Video t·∫£i l√™n", "üé• Video tr·ª±c ti·∫øp"])
    
    # Process based on selected mode
    if mode == "üì∏ ·∫¢nh tƒ©nh":
        # Image input methods
        input_method = st.radio("Ch·ªçn ngu·ªìn ·∫£nh:", ["Upload ·∫£nh", "Ch·ª•p t·ª´ webcam"])
        
        if input_method == "Ch·ª•p t·ª´ webcam":
            # Camera capture interface  
            picture = st.camera_input("Ch·ª•p ·∫£nh khu√¥n m·∫∑t")
            
            if picture:
                # Convert to OpenCV format
                bytes_data = picture.getvalue()
                img = cv2.imdecode(np.frombuffer(bytes_data, np.uint8), cv2.IMREAD_COLOR)
                
                # Create two columns for original and result images
                col1, col2 = st.columns(2)
                
                # Display original image
                with col1:
                    st.subheader("·∫¢nh g·ªëc")
                    st.image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), use_container_width=True)
                
                # Process image
                with st.spinner("ƒêang nh·∫≠n d·∫°ng khu√¥n m·∫∑t..."):
                    faces, aligned_faces = face_detector.detect(img)
                    
                    if len(faces) > 0:
                        # S·∫Øp x·∫øp khu√¥n m·∫∑t theo k√≠ch th∆∞·ªõc (l·ªõn -> nh·ªè)
                        face_sizes = [f[2] * f[3] for f in faces]  # width * height
                        sorted_indices = np.argsort(face_sizes)[::-1]  # Gi·∫£m d·∫ßn
                        
                        sorted_faces = faces[sorted_indices]
                        sorted_aligned_faces = [aligned_faces[i] for i in sorted_indices if i < len(aligned_faces)]
                        
                        names = []
                        scores = []
                        
                        # Nh·∫≠n di·ªán khu√¥n m·∫∑t (limit to 5)
                        for face_img in sorted_aligned_faces[:5]:
                            # Ch·ªâ s·ª≠ d·ª•ng ·∫£nh g·ªëc, lo·∫°i b·ªè ph∆∞∆°ng ph√°p flip
                            name, score = face_recognizer.identify(face_img, threshold=0.45)
                            names.append(name)
                            scores.append(score)
                        
                        # Draw results - s·ª≠ d·ª•ng h√†m ƒë∆°n gi·∫£n h√≥a
                        result_img = draw_results(img, sorted_faces[:5], names, scores)
                    else:
                        result_img = img  # No faces detected
                        names = []
                        scores = []
                    
                    # Display result
                    with col2:
                        st.subheader("K·∫øt qu·∫£")
                        st.image(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB), use_container_width=True)
                    
                    # Display information below images
                    if names:
                        st.markdown("### Th√¥ng tin nh·∫≠n d·∫°ng:")
                        for i, (name, score) in enumerate(zip(names, scores)):
                            status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if name != "Unknown" else "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c"
                            st.write(f"**Ng∆∞·ªùi {i+1}:** {name} ({score:.2f}) - {status}")
                    else:
                        st.warning("Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t n√†o trong ·∫£nh!")
        
        else:  # Upload image mode
            uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh ch·ª©a khu√¥n m·∫∑t", type=["jpg", "jpeg", "png"])
            
            if uploaded_file is not None:
                # Read image
                file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
                img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
                
                # Create two columns for original and result images
                col1, col2 = st.columns(2)
                
                # Display original image
                with col1:
                    st.subheader("·∫¢nh g·ªëc")
                    st.image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), use_container_width=True)
                
                # Process image
                with st.spinner("ƒêang nh·∫≠n d·∫°ng khu√¥n m·∫∑t..."):
                    faces, aligned_faces = face_detector.detect(img)
                    
                    if len(faces) > 0:
                        # S·∫Øp x·∫øp khu√¥n m·∫∑t theo k√≠ch th∆∞·ªõc (l·ªõn -> nh·ªè)
                        face_sizes = [f[2] * f[3] for f in faces]  # width * height
                        sorted_indices = np.argsort(face_sizes)[::-1]  # Gi·∫£m d·∫ßn
                        
                        sorted_faces = faces[sorted_indices]
                        sorted_aligned_faces = [aligned_faces[i] for i in sorted_indices if i < len(aligned_faces)]
                        
                        names = []
                        scores = []
                        
                        # Nh·∫≠n di·ªán khu√¥n m·∫∑t (limit to 5)
                        for face_img in sorted_aligned_faces[:5]:
                            # Ch·ªâ s·ª≠ d·ª•ng ·∫£nh g·ªëc, lo·∫°i b·ªè ph∆∞∆°ng ph√°p flip
                            name, score = face_recognizer.identify(face_img, threshold=0.45)
                            names.append(name)
                            scores.append(score)
                        
                        # Draw results - s·ª≠ d·ª•ng h√†m ƒë∆°n gi·∫£n h√≥a
                        result_img = draw_results(img, sorted_faces[:5], names, scores)
                    else:
                        result_img = img  # No faces detected
                        names = []
                        scores = []
                    
                    # Display result
                    with col2:
                        st.subheader("K·∫øt qu·∫£")
                        st.image(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB), use_container_width=True)
                    
                    # Display information below images
                    if names:
                        st.markdown("### Th√¥ng tin nh·∫≠n d·∫°ng:")
                        for i, (name, score) in enumerate(zip(names, scores)):
                            status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if name != "Unknown" else "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c"
                            st.write(f"**Ng∆∞·ªùi {i+1}:** {name} ({score:.2f}) - {status}")
                    else:
                        st.warning("Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t n√†o trong ·∫£nh!")
        
    elif mode == "üé¨ Video t·∫£i l√™n":
        # Video upload section with improved explanation
        st.subheader("Nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ video")
        
        # Add helpful explanation
        st.markdown("""
        T√≠nh nƒÉng n√†y cho ph√©p nh·∫≠n d·∫°ng khu√¥n m·∫∑t t·ª´ file video t·∫£i l√™n.
        Video s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω tr·ª±c ti·∫øp v·ªõi k·∫øt qu·∫£ nh·∫≠n d·∫°ng hi·ªÉn th·ªã ngay tr√™n video!
        """)
        
        # Upload video file with clearer instructions
        video_file = st.file_uploader(
            "T·∫£i l√™n video ch·ª©a khu√¥n m·∫∑t c·∫ßn nh·∫≠n d·∫°ng", 
            type=["mp4", "mov", "avi", "mkv", "wmv"],
            help="H·ªó tr·ª£ c√°c ƒë·ªãnh d·∫°ng: MP4, MOV, AVI, MKV, WMV. N√™n s·ª≠ d·ª•ng video MP4 ƒë·ªÉ c√≥ hi·ªáu su·∫•t t·ªët nh·∫•t."
        )
        
        if video_file is not None:
            # S·ª≠ d·ª•ng h√†m x·ª≠ l√Ω video ƒë√£ c·∫£i ti·∫øn
            process_video_realtime(video_file, face_detector, face_recognizer)
            
    elif mode == "üé• Video tr·ª±c ti·∫øp":
        st.markdown("### üé• Video tr·ª±c ti·∫øp t·ª´ camera")
        
        # Performance settings
        col1, col2 = st.columns(2)
        with col1:
            resolution = st.selectbox("ƒê·ªô ph√¢n gi·∫£i:", 
                                    ["640x480", "800x600", "1280x720"],
                                    index=0)
        with col2:
            process_rate = st.slider("T·ªëc ƒë·ªô x·ª≠ l√Ω:", 1, 6, 2, 1,
                                    help="S·ªë frame b·ªè qua gi·ªØa c√°c l·∫ßn x·ª≠ l√Ω (cao h∆°n = nhanh h∆°n)")
        
        # Parse resolution
        w, h = map(int, resolution.split('x'))
        
        # Control buttons
        col1, col2 = st.columns([1, 4])
        with col1:
            start_button = st.button("B·∫Øt ƒë·∫ßu", type="primary", use_container_width=True)
        with col2:
            stop_button = st.button("D·ª´ng", use_container_width=True, key="stop_button")
        
        # Initialize session state
        if 'video_running' not in st.session_state:
            st.session_state.video_running = False
        
        # Video display area
        video_placeholder = st.empty()
        info_placeholder = st.empty()
        
        # Start video stream
        if start_button:
            st.session_state.video_running = True
        
        if stop_button:
            st.session_state.video_running = False
        
        # Video processing loop
        if st.session_state.video_running:
            # Hi·ªÉn th·ªã loading state
            video_placeholder.markdown("### ‚è≥ ƒêang m·ªü camera...")
            
            # Try different camera indices
            cap = None
            for idx in range(3):  # Try index 0, 1, 2
                cap = cv2.VideoCapture(idx)
                if cap.isOpened():
                    break
                cap.release()
            
            if cap is None or not cap.isOpened():
                st.error("Kh√¥ng th·ªÉ m·ªü camera. Vui l√≤ng ki·ªÉm tra l·∫°i k·∫øt n·ªëi camera.")
            else:
                # Set camera resolution
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, w)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, h)
                cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                
                # Hi·ªÉn th·ªã th√†nh c√¥ng
                video_placeholder.markdown("‚úÖ Camera ƒë√£ s·∫µn s√†ng")
                time.sleep(0.5)  # Hi·ªÉn th·ªã th√¥ng b√°o trong 0.5 gi√¢y
                
                # FPS counter
                fps_start_time = time.time()
                fps_counter = 0
                current_fps = 0
                frame_num = 0
                
                while st.session_state.video_running:
                    ret, frame = cap.read()
                    
                    if not ret:
                        st.error("Kh√¥ng th·ªÉ ƒë·ªçc khung h√¨nh t·ª´ camera!")
                        break
                    
                    # Flip frame horizontally for mirror effect
                    frame = cv2.flip(frame, 1)
                    
                    # Process every Nth frame based on speed setting
                    if frame_num % process_rate == 0:
                        # Detect faces
                        faces, aligned_faces = face_detector.detect(frame)
                        
                        # N·∫øu ph√°t hi·ªán c√°c khu√¥n m·∫∑t, s·∫Øp x·∫øp ch√∫ng theo k√≠ch th∆∞·ªõc
                        if len(faces) > 0:
                            face_sizes = [f[2] * f[3] for f in faces]  # width * height
                            sorted_indices = np.argsort(face_sizes)[::-1]  # Gi·∫£m d·∫ßn
                            
                            sorted_faces = faces[sorted_indices]
                            sorted_aligned_faces = [aligned_faces[i] for i in sorted_indices if i < len(aligned_faces)]
                            
                            # Nh·∫≠n di·ªán khu√¥n m·∫∑t
                            names = []
                            scores = []
                            
                            for face_img in sorted_aligned_faces[:5]:  # Ch·ªâ x·ª≠ l√Ω t·ªëi ƒëa 5 khu√¥n m·∫∑t
                                # Ch·ªâ s·ª≠ d·ª•ng ·∫£nh g·ªëc, lo·∫°i b·ªè ph∆∞∆°ng ph√°p flip
                                name, score = face_recognizer.identify(face_img, threshold=0.45)
                                names.append(name)
                                scores.append(score)
                            
                            # V·∫Ω k·∫øt qu·∫£
                            display_frame = draw_results(frame, sorted_faces, names, scores)
                            
                            # Display recognition info
                            if len(names) > 0:
                                info_text = "### Khu√¥n m·∫∑t ƒë∆∞·ª£c nh·∫≠n di·ªán:\n\n"
                                
                                for i, (name, score) in enumerate(zip(names, scores)):
                                    status = "‚úÖ ƒê√£ nh·∫≠n d·∫°ng" if name != "Unknown" else "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c"
                                    info_text += f"**Ng∆∞·ªùi {i+1}:** {name} ({score:.2f}) - {status}\n\n"
                                
                                info_placeholder.markdown(info_text)
                        else:
                            display_frame = frame
                            if frame_num % (process_rate * 5) == 0:
                                info_placeholder.markdown("*Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t n√†o*")
                    else:
                        display_frame = frame
                    
                    frame_num += 1
                    
                    # Calculate FPS
                    fps_counter += 1
                    if time.time() - fps_start_time >= 1.0:
                        current_fps = fps_counter
                        fps_counter = 0
                        fps_start_time = time.time()
                    
                    # Add FPS text to frame
                    cv2.putText(display_frame, f"FPS: {current_fps}", (10, 30),
                              cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                    
                    # Display frame
                    video_placeholder.image(cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB), 
                                          channels="RGB", use_container_width=True)
                    
                    # Check if stop button was pressed
                    if not st.session_state.video_running:
                        break
                
                # Release camera
                cap.release()
                video_placeholder.markdown("**Camera ƒë√£ ƒë∆∞·ª£c ƒë√≥ng**")
        
        elif not st.session_state.video_running and not start_button:
            video_placeholder.markdown("""
            ## üé• Ch·∫ø ƒë·ªô Video Tr·ª±c Ti·∫øp
            
            **T√≠nh nƒÉng:**
            - Hi·ªÉn th·ªã khu√¥n m·∫∑t trong th·ªùi gian th·ª±c
            - T·ª± ƒë·ªông nh·∫≠n d·∫°ng khu√¥n m·∫∑t trong th·ªùi gian th·ª±c
            - Hi·ªÉn th·ªã FPS ƒë·ªÉ theo d√µi hi·ªáu su·∫•t
            
            **T√πy ch·ªçn hi·ªáu su·∫•t:**
            - ƒê·ªô ph√¢n gi·∫£i th·∫•p = nhanh h∆°n
            - T·ªëc ƒë·ªô x·ª≠ l√Ω cao = x·ª≠ l√Ω nhi·ªÅu frame h∆°n/gi√¢y
            
            Nh·∫•n **B·∫Øt ƒë·∫ßu** ƒë·ªÉ m·ªü camera.
            """)
            
def process_video_realtime(video_file, face_detector, face_recognizer):
    """
    C·∫£i ti·∫øn x·ª≠ l√Ω video theo th·ªùi gian th·ª±c - tr·ª±c ti·∫øp hi·ªÉn th·ªã k·∫øt qu·∫£ nh·∫≠n di·ªán l√™n video
    """
    import tempfile
    import os
    import cv2
    import numpy as np
    import time
    from datetime import timedelta
    import streamlit as st
    
    # T·∫°o file t·∫°m
    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmpfile:
        temp_video_path = tmpfile.name
        # L∆∞u video v√†o file t·∫°m
        tmpfile.write(video_file.getbuffer())
    
    # M·ªü video
    cap = cv2.VideoCapture(temp_video_path)
    if not cap.isOpened():
        st.error("Kh√¥ng th·ªÉ ƒë·ªçc video. Vui l√≤ng th·ª≠ v·ªõi ƒë·ªãnh d·∫°ng kh√°c.")
        try:
            os.remove(temp_video_path)
        except:
            pass
        return
    
    # L·∫•y th√¥ng tin video
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    duration = frame_count / fps if fps > 0 else 0
    
    # Hi·ªÉn th·ªã th√¥ng tin video
    st.info(f"Th√¥ng tin video: {width}x{height}, {fps:.1f} FPS, {frame_count} frames, th·ªùi l∆∞·ª£ng: {timedelta(seconds=duration)}")
    
    # ƒêi·ªÅu ch·ªânh ƒë·ªô nh·∫°y v√† t·ªëc ƒë·ªô x·ª≠ l√Ω
    col1, col2 = st.columns(2)
    with col1:
        confidence_threshold = st.slider(
            "Ng∆∞·ª°ng nh·∫≠n di·ªán:", 
            min_value=0.4, 
            max_value=0.95, 
            value=0.45, 
            step=0.05,
            help="ƒêi·ªÅu ch·ªânh ƒë·ªô nh·∫°y khi nh·∫≠n di·ªán (cao h∆°n = √≠t nh·∫≠n di·ªán sai h∆°n)"
        )
    with col2:
        processing_speed = st.slider(
            "T·ªëc ƒë·ªô x·ª≠ l√Ω:", 
            min_value=1, 
            max_value=10, 
            value=3, 
            step=1,
            help="T·ªëc ƒë·ªô x·ª≠ l√Ω (cao h∆°n = nhanh h∆°n nh∆∞ng b·ªè qua nhi·ªÅu frame)"
        )
    
    # Thi·∫øt l·∫≠p video player v√† controls
    video_placeholder = st.empty()
    progress_bar = st.progress(0.0)
    info_display = st.empty()
    
    # Controls
    col1, col2, col3 = st.columns([1, 1, 1])
    with col1:
        start_button = st.button("‚ñ∂Ô∏è B·∫Øt ƒë·∫ßu", type="primary", use_container_width=True)
    with col2:
        pause_button = st.button("‚è∏Ô∏è T·∫°m d·ª´ng", use_container_width=True)
    with col3:
        stop_button = st.button("‚èπÔ∏è D·ª´ng", use_container_width=True)
    
    # Th√¥ng tin nh·∫≠n di·ªán
    stats = {
        'people_detected': set(),
        'people_frames': {},
        'total_faces': 0,
        'identified_faces': 0,
        'unknown_faces': 0
    }
    
    # Theo d√µi tr·∫°ng th√°i
    if 'video_playing' not in st.session_state:
        st.session_state.video_playing = False
    if 'video_paused' not in st.session_state:
        st.session_state.video_paused = False
    if 'current_frame' not in st.session_state:
        st.session_state.current_frame = 0
        
    # X·ª≠ l√Ω n√∫t b·∫•m
    if start_button:
        st.session_state.video_playing = True
        st.session_state.video_paused = False
        if st.session_state.current_frame >= frame_count:
            st.session_state.current_frame = 0
            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
    
    if pause_button:
        st.session_state.video_paused = not st.session_state.video_paused
    
    if stop_button:
        st.session_state.video_playing = False
        st.session_state.video_paused = False
        st.session_state.current_frame = 0
        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
    
    # X·ª≠ l√Ω ph√°t video
    if st.session_state.video_playing:
        # ƒê·∫∑t v·ªã tr√≠ frame n·∫øu c·∫ßn
        if st.session_state.current_frame > 0:
            cap.set(cv2.CAP_PROP_POS_FRAMES, st.session_state.current_frame)
        
        # Bi·∫øn theo d√µi t·ªëc ƒë·ªô FPS
        start_time = time.time()
        processed_frames = 0
        display_fps = 0
        
        # T·∫°o file video output t·∫°m th·ªùi
        temp_output_path = temp_video_path + "_output.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = None  # S·∫Ω ƒë∆∞·ª£c kh·ªüi t·∫°o sau
        
        # Chu·∫©n b·ªã th√¥ng tin hi·ªán th·ªã
        info_text = ""
        
        try:
            while st.session_state.video_playing and st.session_state.current_frame < frame_count:
                # Ki·ªÉm tra t·∫°m d·ª´ng
                if st.session_state.video_paused:
                    time.sleep(0.1)  # T·∫°m d·ª´ng ng·∫Øn ƒë·ªÉ tr√°nh treo
                    continue
                
                # ƒê·ªçc frame
                ret, frame = cap.read()
                if not ret:
                    break
                
                st.session_state.current_frame += 1
                
                # Ch·ªâ x·ª≠ l√Ω m·ªôt s·ªë frame (b·ªè qua frame ƒë·ªÉ tƒÉng t·ªëc)
                if st.session_state.current_frame % processing_speed != 0:
                    continue
                
                # Ph√°t hi·ªán khu√¥n m·∫∑t
                faces, aligned_faces = face_detector.detect(frame)
                
                # C·∫≠p nh·∫≠t th·ªëng k√™
                if len(faces) > 0:
                    stats['total_faces'] += len(faces)
                    
                    # S·∫Øp x·∫øp khu√¥n m·∫∑t theo k√≠ch th∆∞·ªõc (l·ªõn -> nh·ªè)
                    face_sizes = [f[2] * f[3] for f in faces]  # width * height
                    sorted_indices = np.argsort(face_sizes)[::-1]  # Gi·∫£m d·∫ßn
                    
                    sorted_faces = faces[sorted_indices]
                    sorted_aligned_faces = [aligned_faces[i] for i in sorted_indices if i < len(aligned_faces)]
                    
                    names = []
                    scores = []
                    
                    # Nh·∫≠n di·ªán khu√¥n m·∫∑t (gi·ªõi h·∫°n 5 khu√¥n m·∫∑t)
                    for face_img in sorted_aligned_faces[:5]:
                        name, score = face_recognizer.identify(face_img, threshold=confidence_threshold)
                        
                        # C·∫≠p nh·∫≠t th·ªëng k√™
                        if name != "Unknown":
                            stats['identified_faces'] += 1
                            stats['people_detected'].add(name)
                            stats['people_frames'][name] = stats['people_frames'].get(name, 0) + 1
                        else:
                            stats['unknown_faces'] += 1
                            
                        names.append(name)
                        scores.append(score)
                    
                    # V·∫Ω k·∫øt qu·∫£
                    result_frame = draw_results(frame, sorted_faces[:5], names, scores)
                else:
                    result_frame = frame
                
                # T√≠nh FPS
                processed_frames += 1
                elapsed_time = time.time() - start_time
                if elapsed_time >= 1.0:  # C·∫≠p nh·∫≠t FPS m·ªói gi√¢y
                    display_fps = processed_frames / elapsed_time
                    processed_frames = 0
                    start_time = time.time()
                
                # Th√™m th√¥ng tin FPS v√†o frame
                cv2.putText(
                    result_frame,
                    f"FPS: {display_fps:.1f}",
                    (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    1.0,
                    (0, 255, 0),
                    2
                )
                
                # Th√™m th√¥ng tin v·ªã tr√≠ frame
                position_percent = st.session_state.current_frame / frame_count
                cv2.putText(
                    result_frame,
                    f"Frame: {st.session_state.current_frame}/{frame_count}",
                    (10, height - 10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.8,
                    (0, 255, 0),
                    2
                )
                
                # C·∫≠p nh·∫≠t thanh ti·∫øn tr√¨nh
                progress_bar.progress(position_percent)
                
                # Hi·ªÉn th·ªã frame ƒë√£ x·ª≠ l√Ω
                video_placeholder.image(
                    cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB),
                    channels="RGB",
                    use_container_width=True
                )
                
                # Kh·ªüi t·∫°o output writer n·∫øu c·∫ßn
                if out is None:
                    out = cv2.VideoWriter(
                        temp_output_path,
                        fourcc,
                        fps,
                        (width, height)
                    )
                
                # L∆∞u frame v√†o video output
                out.write(result_frame)
                
                # Hi·ªÉn th·ªã th√¥ng tin nh·∫≠n di·ªán hi·ªán t·∫°i
                if len(stats['people_detected']) > 0:
                    info_text = f"**Nh·∫≠n di·ªán ƒë∆∞·ª£c:** {len(stats['people_detected'])} ng∆∞·ªùi"
                    info_text += f" | **Khu√¥n m·∫∑t ƒë√£ x·ª≠ l√Ω:** {stats['total_faces']}"
                    info_text += f" | **ƒê√£ nh·∫≠n di·ªán:** {stats['identified_faces']}"
                    info_text += f" | **Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c:** {stats['unknown_faces']}"
                    
                    # Top 3 ng∆∞·ªùi xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
                    if stats['people_frames']:
                        sorted_people = sorted(stats['people_frames'].items(), key=lambda x: x[1], reverse=True)
                        top_people = sorted_people[:3]
                        info_text += "\n\n**Ng∆∞·ªùi xu·∫•t hi·ªán nhi·ªÅu nh·∫•t:**"
                        for name, count in top_people:
                            info_text += f" {name} ({count} frames),"
                        info_text = info_text.rstrip(",")
                
                info_display.markdown(info_text)
                
                # Ki·ªÉm tra tr·∫°ng th√°i
                if not st.session_state.video_playing or st.session_state.current_frame >= frame_count:
                    break
                
            # ƒê√≥ng video writer
            if out is not None:
                out.release()
            
            # Hi·ªÉn th·ªã th√¥ng b√°o k·∫øt th√∫c n·∫øu ƒë√£ xem h·∫øt video
            if st.session_state.current_frame >= frame_count:
                st.success("ƒê√£ ho√†n th√†nh ph√°t video!")
                st.session_state.video_playing = False
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£ cu·ªëi c√πng
                st.subheader("K·∫øt qu·∫£ nh·∫≠n di·ªán")
                
                # Hi·ªÉn th·ªã c√°c ng∆∞·ªùi ƒë∆∞·ª£c nh·∫≠n di·ªán 
                if stats['people_detected']:
                    st.markdown("### Ng∆∞·ªùi xu·∫•t hi·ªán trong video:")
                    
                    # S·∫Øp x·∫øp theo s·ªë l·∫ßn xu·∫•t hi·ªán
                    if stats['people_frames']:
                        people_cols = st.columns(min(3, len(stats['people_frames'])))
                        sorted_people = sorted(stats['people_frames'].items(), key=lambda x: x[1], reverse=True)
                        
                        for i, (person, frames) in enumerate(sorted_people):
                            with people_cols[i % 3]:
                                frame_percent = frames / frame_count * 100
                                st.metric(
                                    label=person,
                                    value=f"{frames} frames",
                                    delta=f"{frame_percent:.1f}%"
                                )
                else:
                    st.warning("Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c ng∆∞·ªùi n√†o trong video.")
                
                # T·∫£i v·ªÅ video ƒë√£ x·ª≠ l√Ω
                if os.path.exists(temp_output_path):
                    with open(temp_output_path, 'rb') as f:
                        video_bytes = f.read()
                        
                    st.download_button(
                        label="T·∫£i xu·ªëng video ƒë√£ x·ª≠ l√Ω",
                        data=video_bytes,
                        file_name="video_with_face_recognition.mp4",
                        mime="video/mp4"
                    )
        
        except Exception as e:
            st.error(f"L·ªói khi x·ª≠ l√Ω video: {str(e)}")
        
        finally:
            # Gi·∫£i ph√≥ng resources
            cap.release()
            if out is not None:
                out.release()
    
    else:
        # Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n khi kh√¥ng ph√°t
        video_placeholder.markdown("""
        ## üé¨ X·ª≠ l√Ω video nh·∫≠n di·ªán khu√¥n m·∫∑t
        
        T·∫£i l√™n video c·ªßa b·∫°n v√† nh·∫•n **‚ñ∂Ô∏è B·∫Øt ƒë·∫ßu** ƒë·ªÉ b·∫Øt ƒë·∫ßu x·ª≠ l√Ω.
        
        ·ª®ng d·ª•ng s·∫Ω ph√°t hi·ªán v√† nh·∫≠n di·ªán khu√¥n m·∫∑t trong video, hi·ªÉn th·ªã tr·ª±c ti·∫øp k·∫øt qu·∫£ tr√™n m√†n h√¨nh.
        
        **T√≠nh nƒÉng:**
        - Hi·ªÉn th·ªã video v·ªõi nh·∫≠n di·ªán khu√¥n m·∫∑t theo th·ªùi gian th·ª±c
        - Theo d√µi s·ªë l∆∞·ª£ng khu√¥n m·∫∑t ƒë∆∞·ª£c nh·∫≠n di·ªán
        - ƒêi·ªÅu ch·ªânh ƒë·ªô nh·∫°y v√† t·ªëc ƒë·ªô x·ª≠ l√Ω
        - T·∫£i xu·ªëng video ƒë√£ x·ª≠ l√Ω
        """)
    
    # D·ªçn d·∫πp
    try:
        if os.path.exists(temp_video_path):
            os.remove(temp_video_path)
        if os.path.exists(temp_output_path):
            os.remove(temp_output_path)
    except:
        pass